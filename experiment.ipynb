{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "experiment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ultronify/atari-dqn/blob/master/experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w58OjrKzOIO9",
        "colab_type": "text"
      },
      "source": [
        "# Play Atari games with DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyPM8KWNOIO-",
        "colab_type": "text"
      },
      "source": [
        "This notebook shows how to play image based Atari games (will leave out the RAM games since the pipeline can be very different) with DQN agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUNZPpFrOVfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "colab = True\n",
        "\n",
        "base_dir = './'\n",
        "\n",
        "if colab:\n",
        "  from google.colab import drive\n",
        "  gdrive_dir = '/content/gdrive';\n",
        "  drive.mount(gdrive_dir)\n",
        "  base_dir = os.path.join(gdrive_dir, 'My\\ Drive', 'github_colab_projects', 'dqn-atari')\n",
        "  !pip install tf_agents gym[atari]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLDSAorpOIO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM7VyonlOIPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Qnet(Model):\n",
        "    def __init__(self, action_space_size, state_shape):\n",
        "      super(Qnet, self).__init__()\n",
        "      self.action_space_size = action_space_size\n",
        "      self.state_shape = state_shape\n",
        "      # Define conv layers\n",
        "      self.conv_in = Conv2D(32, 8, strides=4, activation='relu', input_shape=self.state_shape)\n",
        "      self.conv_0 = Conv2D(64, 4, strides=2, activation='relu')\n",
        "      self.conv_1 = Conv2D(64, 3, strides=1, activation='relu')\n",
        "      self.flatten = Flatten()\n",
        "      # Dense layers\n",
        "      self.dense_0 = Dense(512, activation='relu')\n",
        "      self.dense_out = Dense(self.action_space_size, activation='linear')\n",
        "    \n",
        "    def call(self, inputs):\n",
        "      x = inputs\n",
        "      x = self.conv_in(x)\n",
        "      x = self.conv_0(x)\n",
        "      x = self.conv_1(x)\n",
        "      x = self.flatten(x)\n",
        "      x = self.dense_0(x)\n",
        "      x = self.dense_out(x)\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuYRdiKYOIPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_length, state_shape, action_space_size):\n",
        "      self.max_length = max_length\n",
        "      self.state_shape = state_shape\n",
        "      self.action_space_size = action_space_size\n",
        "      self.replay_buffer = deque(maxlen=max_length)\n",
        "    \n",
        "    def get_size(self):\n",
        "      return len(self.replay_buffer)\n",
        "    \n",
        "    def add_batch(self, batch):\n",
        "      self.replay_buffer.append(batch)\n",
        "    \n",
        "    def get_batch(self, batch_size):\n",
        "      batch = random.sample(self.replay_buffer, batch_size)\n",
        "      states_batch = []\n",
        "      q_vals_batch = []\n",
        "      for s in batch:\n",
        "        state, q_vals = s\n",
        "        states_batch.append(state)\n",
        "        q_vals_batch.append(q_vals)\n",
        "      return tf.convert_to_tensor(states_batch, dtype=tf.float32), tf.convert_to_tensor(q_vals_batch, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmsASrjxOIPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DqnAgent:    \n",
        "    def __init__(self, action_space_size, state_shape, train_per_eps,\n",
        "                 learning_rate, checkpoint_location, update_interval):\n",
        "      self.action_space_size = action_space_size\n",
        "      self.state_shape = state_shape\n",
        "      self.update_interval = update_interval\n",
        "      self.batch_cnt = 0\n",
        "      self.learning_rate = learning_rate\n",
        "      self.train_per_eps = train_per_eps\n",
        "      self.checkpoint_location = checkpoint_location\n",
        "      self.q_net = self.create_q_net()\n",
        "      self.target_q_net = self.create_q_net()\n",
        "      self.checkpoint = tf.train.Checkpoint(step=tf.Variable(1), net=self.q_net)\n",
        "      self.checkpoint_manager = tf.train.CheckpointManager(\n",
        "          self.checkpoint, self.checkpoint_location, max_to_keep=3)\n",
        "      try:\n",
        "          self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
        "          self.update()\n",
        "      except:\n",
        "          self.q_net = self.create_q_net()\n",
        "          print('Checkpoint not found, initialize new model')\n",
        "        \n",
        "    def update(self):\n",
        "      self.target_q_net.set_weights(self.q_net.get_weights())\n",
        "        \n",
        "    def create_q_net(self):\n",
        "      q_net = Qnet(self.action_space_size, self.state_shape)\n",
        "      # q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss=tf.keras.losses.Huber())\n",
        "      # q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "      q_net.compile(optimizer=tf.optimizers.RMSprop(learning_rate=self.learning_rate), loss='mse')\n",
        "      return q_net\n",
        "    \n",
        "    def random_policy(self, state):\n",
        "      return np.random.randint(self.action_space_size)\n",
        "    \n",
        "    def net_policy(self, state):\n",
        "      q_vals = self.get_q_vals(state).numpy()[0]\n",
        "      action = np.argmax(q_vals)\n",
        "      return action\n",
        "    \n",
        "    def collect_policy(self, state, epsilon):\n",
        "      if np.random.random() < epsilon:\n",
        "        return self.random_policy(state)\n",
        "      return self.net_policy(state)\n",
        "    \n",
        "    @tf.function\n",
        "    def get_next_q_vals(self, state):\n",
        "      next_q_vals = self.target_q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "      return next_q_vals\n",
        "    \n",
        "    @tf.function\n",
        "    def get_q_vals(self, state):\n",
        "      q_vals = self.q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "      return q_vals\n",
        "    \n",
        "    def train(self, replay_buffer, batch_size):\n",
        "      loss = []\n",
        "      for i in range(self.train_per_eps):\n",
        "        states, target_q_vals = replay_buffer.get_batch(batch_size)\n",
        "        result = self.q_net.fit(x=states, y=target_q_vals, verbose=1)\n",
        "        loss.append(result.history['loss'])\n",
        "        self.batch_cnt += 1\n",
        "        if self.batch_cnt % self.update_interval == 0:\n",
        "          print('Finished {0} ({1}%). Update target net.'.format(i, i / self.train_per_eps * 100.0))\n",
        "          self.update()\n",
        "        self.checkpoint_manager.save()\n",
        "      return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KZ1OSfxOIPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GameEnv:\n",
        "    def __init__(self, game_id, n_frames, max_noop_steps):\n",
        "        self.game_id = game_id\n",
        "        self.n_frames = n_frames\n",
        "        self.max_noop_steps = max_noop_steps\n",
        "        self.env = gym.make(game_id)\n",
        "        self.frames = deque(maxlen=self.n_frames)\n",
        "        self.init_state = None\n",
        "        self.still_doing_nothing = True\n",
        "        self.current_step = 0\n",
        "        \n",
        "    def sanitize_state(self, state, downsample=2):\n",
        "        img = Image.fromarray(state).convert('L')\n",
        "        w, h = img.size\n",
        "        w, h = w // 2, h // 2\n",
        "        img = np.array(img.resize((w, h)))\n",
        "        normalized_img = img / 255.0\n",
        "        return normalized_img\n",
        "    \n",
        "    def normalize_reward(self, reward):\n",
        "        if reward > 0:\n",
        "          return 1.0\n",
        "        elif reward < 0:\n",
        "          return -1.0\n",
        "        else:\n",
        "          return 0.0\n",
        "        \n",
        "    def get_action_space_size(self):\n",
        "        return self.env.action_space.n\n",
        "    \n",
        "    def get_state_shape(self):\n",
        "        initial_state = self.reset()\n",
        "        return initial_state.shape\n",
        "        \n",
        "    def render(self):\n",
        "        self.env.render()\n",
        "        \n",
        "    def get_frames(self):\n",
        "        frames = np.array(self.frames)\n",
        "        frames = np.swapaxes(frames, 0, 2)\n",
        "        return frames\n",
        "    \n",
        "    def doing_noting(self, state):\n",
        "        if self.current_step == self.max_noop_steps:\n",
        "          return self.still_doing_nothing\n",
        "        elif self.current_step < self.max_noop_steps:\n",
        "          if self.still_doing_nothing and not np.array_equal(np.array(state), self.init_state):\n",
        "              self.still_doing_nothing = False\n",
        "        else:\n",
        "          return False\n",
        "        \n",
        "    def reset(self):\n",
        "        unsanitized_initial_state = self.env.reset()\n",
        "        self.init_state = np.array(unsanitized_initial_state)\n",
        "        self.current_step = 0\n",
        "        self.still_doing_nothing = True\n",
        "        initial_state = self.sanitize_state(unsanitized_initial_state)\n",
        "        for _ in range(self.n_frames):\n",
        "          self.frames.append(initial_state)\n",
        "        return self.get_frames()\n",
        "    \n",
        "    def step(self, action):\n",
        "        unsanitized_state, unnormalized_reward, done, info = self.env.step(action)\n",
        "        if self.doing_noting(unsanitized_state):\n",
        "          done = True\n",
        "        state = self.sanitize_state(unsanitized_state)\n",
        "        self.frames.append(state)\n",
        "        reward = self.normalize_reward(unnormalized_reward)\n",
        "        self.current_step += 1\n",
        "        return self.get_frames(), reward, done, info\n",
        "    \n",
        "    def close(self):\n",
        "        self.env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxtMZ411OIPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_episode(env, agent, epsilon, epsilon_decay_per_step, final_epsilon, gamma, collect, buffer, render):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    total_reward = 0.0\n",
        "    current_epsilon = epsilon\n",
        "    while not done:\n",
        "      if render:\n",
        "        env.render()\n",
        "      action = agent.collect_policy(state, epsilon)\n",
        "      next_state, reward, done, info = env.step(action)\n",
        "      if collect:\n",
        "        q_vals = agent.get_q_vals(state).numpy()[0]\n",
        "        next_q_vals = agent.get_next_q_vals(next_state).numpy()[0]\n",
        "        q_vals[action] = reward\n",
        "        if not done:\n",
        "          q_vals[action] += (next_q_vals.max() * gamma)\n",
        "        buffer.add_batch((state, q_vals))\n",
        "        if current_epsilon >= final_epsilon:\n",
        "          current_epsilon -= epsilon_decay_per_step\n",
        "      state = next_state\n",
        "      total_reward += reward\n",
        "      steps += 1\n",
        "    return total_reward, current_epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKVdgTmzOIPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_tensorflow_gpu():\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "      try:\n",
        "        for gpu in gpus:\n",
        "          tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "      except RuntimeError as e:\n",
        "        print(e)\n",
        "    print('GPU initialized')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-X5p992OIPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps):\n",
        "  fig, (loss_plt, train_plt, eval_plt) = plt.subplots(3, figsize=(20, 8))\n",
        "  fig.suptitle('Eps {0} with epsilon {1}'.format(eps, epsilon))\n",
        "  loss_plt.plot(loss)\n",
        "  loss_plt.set_title('Loss history')\n",
        "  train_plt.plot(train_rewards)\n",
        "  train_plt.set_title('Training reward history')\n",
        "  eval_plt.plot(eval_rewards)\n",
        "  eval_plt.set_title('Eval reward history')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqxY5bN8OIPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_agent(max_eps=200,\n",
        "                max_steps_increase_amount=10,\n",
        "                max_steps_increase_interval=5,\n",
        "                render=False,\n",
        "                max_buffer_len=10000000,\n",
        "                start_buffer_len=50000,\n",
        "                max_noop_steps=30,\n",
        "                batch_size=128,\n",
        "                visualize_interval=1,\n",
        "                train_per_eps=50000,\n",
        "                learning_rate=0.00025,\n",
        "                initial_epsilon=1.0,\n",
        "                final_epsilon=0.1,\n",
        "                epsilon_decay_interval=1000000,\n",
        "                update_interval=10000,\n",
        "                eval_interval=1,\n",
        "                eval_eps=5,\n",
        "                base_dir='./',\n",
        "                agent_id='default',\n",
        "                train_eps=5,\n",
        "                initial_save_threshold=0.0,\n",
        "                game_id='Breakout-v0'):\n",
        "    init_tensorflow_gpu()\n",
        "    train_env = GameEnv(game_id, 4, max_noop_steps)\n",
        "    eval_env = GameEnv(game_id, 4, max_noop_steps)\n",
        "    state_shape = train_env.get_state_shape()\n",
        "    action_space_size = train_env.get_action_space_size()\n",
        "    epsilon = initial_epsilon\n",
        "    buf = ReplayBuffer(max_buffer_len, state_shape, action_space_size)\n",
        "    print('Initialize agent with state shape {0} and action space size {1}'.format(state_shape, action_space_size))\n",
        "    agent = DqnAgent(action_space_size, state_shape,\n",
        "                     train_per_eps, learning_rate,\n",
        "                     os.path.join(base_dir, 'checkpoints', agent_id),\n",
        "                     update_interval)\n",
        "    loss = []\n",
        "    train_rewards = [0.0]\n",
        "    eval_rewards = [0.0]\n",
        "    best_eval_reward = initial_save_threshold\n",
        "    init_buf_steps = 0\n",
        "    epsilon_decay_per_step = (initial_epsilon - final_epsilon) / epsilon_decay_interval\n",
        "    while buf.get_size() < start_buffer_len:\n",
        "      train_reward, epsilon = play_episode(\n",
        "        train_env, agent, render=render,\n",
        "        collect=True, buffer=buf, epsilon=epsilon, gamma=0.95,\n",
        "        epsilon_decay_per_step=epsilon_decay_per_step,\n",
        "        final_epsilon=final_epsilon\n",
        "      )\n",
        "      train_rewards.append(train_reward)\n",
        "      print('Currently filled {0} ({1}%) buffer'.format(buf.get_size(), buf.get_size() / start_buffer_len * 100.0))\n",
        "      init_buf_steps += 1\n",
        "    print('Buffer has been filled with {0} episodes.'.format(init_buf_steps))\n",
        "    for eps in range(1, max_eps + 1):\n",
        "      for i in range(train_eps):\n",
        "        train_reward, epsilon = play_episode(\n",
        "          train_env, agent, render=render,\n",
        "          collect=True, buffer=buf, epsilon=epsilon, gamma=0.95,\n",
        "          epsilon_decay_per_step=epsilon_decay_per_step,\n",
        "          final_epsilon=final_epsilon\n",
        "        )\n",
        "        train_rewards.append(train_reward)\n",
        "        print('Finished {0}/{1}'.format(i + 1, train_eps))\n",
        "      loss += agent.train(buf, batch_size)\n",
        "      if eps % eval_interval == 0:\n",
        "        total_reward = 0.0\n",
        "        for _ in range(eval_eps):\n",
        "          eval_reward, _ = play_episode(\n",
        "            eval_env, agent, render=render,\n",
        "            collect=False, epsilon=0, epsilon_decay_per_step=0,\n",
        "            final_epsilon=0, buffer=None, gamma=0\n",
        "          )\n",
        "          total_reward += eval_reward\n",
        "        avg_reward = total_reward/eval_eps\n",
        "        eval_rewards.append(avg_reward)\n",
        "        if avg_reward >= best_eval_reward:\n",
        "          best_eval_reward = avg_reward\n",
        "          tf.saved_model.save(agent.q_net, os.path.join(base_dir, 'models', agent_id))\n",
        "      if eps % visualize_interval == 0:\n",
        "        visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps)\n",
        "    print('Last saved model has eval result {0}'.format(best_eval_reward))\n",
        "    train_env.close()\n",
        "    eval_env.close()\n",
        "    print('Done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZodMpWS_OIPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_agent(\n",
        "    game_id='Breakout-v0',\n",
        "    max_eps=2000,\n",
        "    render=False,\n",
        "    batch_size=3200,\n",
        "    train_eps=10,\n",
        "    base_dir=base_dir,\n",
        "    epsilon_decay_interval=1000000,\n",
        "    train_per_eps=50,\n",
        "    update_interval=2,\n",
        "    max_buffer_len=30000,\n",
        "    start_buffer_len=15000,\n",
        "    eval_eps=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAXC86bcOIPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}