{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Atari games with DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to play image based Atari games (will leave out the RAM games since the pipeline can be very different) with DQN agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(Model):\n",
    "    def __init__(self, action_space_size, state_shape):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.action_space_size = action_space_size\n",
    "        self.state_shape = state_shape\n",
    "        # Define conv layers\n",
    "        self.conv_in = Conv2D(32, 8, strides=4, activation='relu', input_shape=self.state_shape)\n",
    "        self.conv_0 = Conv2D(64, 4, strides=2, activation='relu')\n",
    "        self.conv_1 = Conv2D(64, 3, strides=1, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        # Dense layers\n",
    "        self.dense_0 = Dense(512, activation='relu')\n",
    "        self.dense_out = Dense(self.action_space_size, activation='linear')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.conv_in(x)\n",
    "        x = self.conv_0(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_0(x)\n",
    "        x = self.dense_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_length, state_shape, action_space_size):\n",
    "        self.max_length = max_length\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space_size = action_space_size\n",
    "        self.state_spec = tf.TensorSpec(state_shape, dtype=tf.float32, name='state')\n",
    "        self.q_vals_spec = tf.TensorSpec([self.action_space_size], dtype=tf.float32, name='q_vals')\n",
    "        self.buffer = TFUniformReplayBuffer(\n",
    "            data_spec=(self.state_spec, self.q_vals_spec),\n",
    "            batch_size=1,\n",
    "            max_length=self.max_length)\n",
    "    \n",
    "    def get_size(self):\n",
    "        return int(self.buffer.num_frames())\n",
    "    \n",
    "    def add_batch(self, batch):\n",
    "        self.buffer.add_batch(batch)\n",
    "    \n",
    "    def get_next(self, sample_batch_size=32):\n",
    "        return self.buffer.get_next(sample_batch_size=sample_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent:    \n",
    "    def __init__(self, action_space_size, state_shape, train_per_eps,\n",
    "                 learning_rate, checkpoint_location, update_interval):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.state_shape = state_shape\n",
    "        self.update_interval = update_interval\n",
    "        self.batch_cnt = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_per_eps = train_per_eps\n",
    "        self.checkpoint_location = checkpoint_location\n",
    "        self.q_net = self.create_q_net()\n",
    "        self.target_q_net = self.create_q_net()\n",
    "        self.checkpoint = tf.train.Checkpoint(step=tf.Variable(1), net=self.q_net)\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(\n",
    "            self.checkpoint, self.checkpoint_location, max_to_keep=3)\n",
    "        try:\n",
    "            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
    "            self.update()\n",
    "        except:\n",
    "            self.q_net = self.create_q_net()\n",
    "            print('Checkpoint not found, initialize new model')\n",
    "        \n",
    "    def update(self):\n",
    "        self.target_q_net.set_weights(self.q_net.get_weights())\n",
    "        \n",
    "    def create_q_net(self):\n",
    "        q_net = Qnet(self.action_space_size, self.state_shape)\n",
    "        # q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss=tf.keras.losses.Huber())\n",
    "        # q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        q_net.compile(optimizer=tf.optimizers.RMSprop(learning_rate=self.learning_rate), loss='mse')\n",
    "        return q_net\n",
    "    \n",
    "    def random_policy(self, state):\n",
    "        return np.random.randint(self.action_space_size)\n",
    "    \n",
    "    def net_policy(self, state):\n",
    "        q_vals = self.get_q_vals(state).numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        return action\n",
    "    \n",
    "    def collect_policy(self, state, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return self.random_policy(state)\n",
    "        return self.net_policy(state)\n",
    "    \n",
    "    def get_next_q_vals(self, state):\n",
    "        next_q_vals = self.target_q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        return next_q_vals\n",
    "    \n",
    "    def get_q_vals(self, state):\n",
    "        q_vals = self.q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        return q_vals\n",
    "    \n",
    "    def train(self, replay_buffer, batch_size):\n",
    "        loss = []\n",
    "        for i in range(self.train_per_eps):\n",
    "            batch, info = replay_buffer.get_next(batch_size)\n",
    "            states, target_q_vals = batch\n",
    "            result = self.q_net.fit(x=states, y=target_q_vals, verbose=0)\n",
    "            loss.append(result.history['loss'])\n",
    "            self.batch_cnt += 1\n",
    "            if self.batch_cnt % self.update_interval == 0:\n",
    "                print('Finished {0} ({1}%). Update target net.'.format(i, i / train_per_eps * 100.0))\n",
    "                self.update()\n",
    "        self.checkpoint_manager.save()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    def __init__(self, game_id, n_frames, max_noop_steps):\n",
    "        self.game_id = game_id\n",
    "        self.n_frames = n_frames\n",
    "        self.max_noop_steps = max_noop_steps\n",
    "        self.env = gym.make(game_id)\n",
    "        self.frames = deque(maxlen=self.n_frames)\n",
    "        self.init_state = None\n",
    "        self.still_doing_nothing = True\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def sanitize_state(self, state, downsample=2):\n",
    "        img = Image.fromarray(state).convert('L')\n",
    "        w, h = img.size\n",
    "        w, h = w // 2, h // 2\n",
    "        img = np.array(img.resize((w, h)))\n",
    "        normalized_img = img / 255.0\n",
    "        return normalized_img\n",
    "    \n",
    "    def normalize_reward(self, reward):\n",
    "        if reward > 0:\n",
    "            return 1.0\n",
    "        elif reward < 0:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "        \n",
    "    def get_action_space_size(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def get_state_shape(self):\n",
    "        initial_state = self.reset()\n",
    "        return initial_state.shape\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "        \n",
    "    def get_frames(self):\n",
    "        frames = np.array(self.frames)\n",
    "        frames = np.swapaxes(frames, 0, 2)\n",
    "        return frames\n",
    "    \n",
    "    def doing_noting(self, state):\n",
    "        if self.current_step == self.max_noop_steps:\n",
    "            return self.still_doing_nothing\n",
    "        elif self.current_step < self.max_noop_steps:\n",
    "            if self.still_doing_nothing and not np.array_equal(np.array(state), self.init_state):\n",
    "                self.still_doing_nothing = False\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def reset(self):\n",
    "        unsanitized_initial_state = self.env.reset()\n",
    "        self.init_state = np.array(unsanitized_initial_state)\n",
    "        self.current_step = 0\n",
    "        self.still_doing_nothing = True\n",
    "        initial_state = self.sanitize_state(unsanitized_initial_state)\n",
    "        for _ in range(self.n_frames):\n",
    "            self.frames.append(initial_state)\n",
    "        return self.get_frames()\n",
    "    \n",
    "    def step(self, action):\n",
    "        unsanitized_state, unnormalized_reward, done, info = self.env.step(action)\n",
    "        if self.doing_noting(unsanitized_state):\n",
    "            done = True\n",
    "        state = self.sanitize_state(unsanitized_state)\n",
    "        self.frames.append(state)\n",
    "        reward = self.normalize_reward(unnormalized_reward)\n",
    "        self.current_step += 1\n",
    "        return self.get_frames(), reward, done, info\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, agent, epsilon, epsilon_decay_per_step, final_epsilon, gamma, collect, buffer, render):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0.0\n",
    "    current_epsilon = epsilon\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.collect_policy(state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if collect:\n",
    "            q_vals = agent.get_q_vals(state).numpy()[0]\n",
    "            next_q_vals = agent.get_next_q_vals(next_state).numpy()[0]\n",
    "            q_vals[action] = reward\n",
    "            if not done:\n",
    "                q_vals[action] += (next_q_vals.max() * gamma)\n",
    "            buffer.add_batch((\n",
    "                tf.convert_to_tensor([state], dtype=tf.float32),\n",
    "                tf.convert_to_tensor([q_vals], dtype=tf.float32)\n",
    "            ))\n",
    "            if current_epsilon >= final_epsilon:\n",
    "                current_epsilon -= epsilon_decay_per_step\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    return total_reward, current_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tensorflow_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    print('GPU initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps):\n",
    "    fig, (loss_plt, train_plt, eval_plt) = plt.subplots(3, figsize=(20, 8))\n",
    "    fig.suptitle('Eps {0} with epsilon {1}'.format(eps, epsilon))\n",
    "    loss_plt.plot(loss)\n",
    "    loss_plt.set_title('Loss history')\n",
    "    train_plt.plot(train_rewards)\n",
    "    train_plt.set_title('Training reward history')\n",
    "    eval_plt.plot(eval_rewards)\n",
    "    eval_plt.set_title('Eval reward history')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(max_eps=200,\n",
    "                max_steps_increase_amount=10,\n",
    "                max_steps_increase_interval=5,\n",
    "                render=False,\n",
    "                max_buffer_len=1000000,\n",
    "                start_buffer_len=50000,\n",
    "                max_noop_steps=30,\n",
    "                batch_size=128,\n",
    "                visualize_interval=1,\n",
    "                train_per_eps=50000,\n",
    "                learning_rate=0.00025,\n",
    "                initial_epsilon=1.0,\n",
    "                final_epsilon=0.1,\n",
    "                epsilon_decay_interval=1000000,\n",
    "                update_interval=10000,\n",
    "                eval_interval=1,\n",
    "                eval_eps=5,\n",
    "                agent_id='default', train_eps=5,\n",
    "                initial_save_threshold=0.0,\n",
    "                game_id='Breakout-v0'):\n",
    "    init_tensorflow_gpu()\n",
    "    train_env = GameEnv(game_id, 4, max_noop_steps)\n",
    "    eval_env = GameEnv(game_id, 4, max_noop_steps)\n",
    "    state_shape = train_env.get_state_shape()\n",
    "    action_space_size = train_env.get_action_space_size()\n",
    "    epsilon = initial_epsilon\n",
    "    buf = ReplayBuffer(max_buffer_len, state_shape, action_space_size)\n",
    "    print('Initialize agent with state shape {0} and action space size {1}'.format(state_shape, action_space_size))\n",
    "    agent = DqnAgent(action_space_size, state_shape,\n",
    "                     train_per_eps, learning_rate,\n",
    "                     os.path.join('./checkpoints', agent_id),\n",
    "                     update_interval)\n",
    "    loss = []\n",
    "    train_rewards = [0.0]\n",
    "    eval_rewards = [0.0]\n",
    "    best_eval_reward = initial_save_threshold\n",
    "    init_buf_steps = 0\n",
    "    epsilon_decay_per_step = (initial_epsilon - final_epsilon) / epsilon_decay_interval\n",
    "    while buf.get_size() < start_buffer_len:\n",
    "        train_reward, epsilon = play_episode(\n",
    "            train_env, agent, render=render,\n",
    "            collect=True, buffer=buf, epsilon=epsilon, gamma=0.95,\n",
    "            epsilon_decay_per_step=epsilon_decay_per_step,\n",
    "            final_epsilon=final_epsilon\n",
    "        )\n",
    "        train_rewards.append(train_reward)\n",
    "        print('Currently filled {0} ({1}%) buffer'.format(buf.get_size(), buf.get_size() / start_buffer_len * 100.0))\n",
    "        init_buf_steps += 1\n",
    "    print('Buffer has been filled with {0} episodes.'.format(init_buf_steps))\n",
    "    for eps in range(1, max_eps + 1):\n",
    "        for _ in range(train_eps):\n",
    "            train_reward, epsilon = play_episode(\n",
    "                train_env, agent, render=render,\n",
    "                collect=True, buffer=buf, epsilon=epsilon, gamma=0.95,\n",
    "                epsilon_decay_per_step=epsilon_decay_per_step,\n",
    "                final_epsilon=final_epsilon\n",
    "            )\n",
    "            train_rewards.append(train_reward)\n",
    "        loss += agent.train(buf, batch_size)\n",
    "        if eps % eval_interval == 0:\n",
    "            total_reward = 0.0\n",
    "            for _ in range(eval_eps):\n",
    "                eval_reward, _ = play_episode(\n",
    "                    eval_env, agent, render=render,\n",
    "                    collect=False, epsilon=0, epsilon_decay_per_step=0,\n",
    "                    final_epsilon=0, buffer=None, gamma=0\n",
    "                )\n",
    "                total_reward += eval_reward\n",
    "            avg_reward = total_reward/eval_eps\n",
    "            eval_rewards.append(avg_reward)\n",
    "            if avg_reward >= best_eval_reward:\n",
    "                best_eval_reward = avg_reward\n",
    "                tf.saved_model.save(agent.q_net, os.path.join('./models', agent_id))\n",
    "        if eps % visualize_interval == 0:\n",
    "            visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps)\n",
    "    print('Last saved model has eval result {0}'.format(best_eval_reward))\n",
    "    train_env.close()\n",
    "    eval_env.close()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "GPU initialized\n",
      "Initialize agent with state shape (80, 105, 4) and action space size 4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-14b41ac9736e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmax_buffer_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mstart_buffer_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     eval_eps=5)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-60593f030114>\u001b[0m in \u001b[0;36mtrain_agent\u001b[1;34m(max_eps, max_steps_increase_amount, max_steps_increase_interval, render, max_buffer_len, start_buffer_len, max_noop_steps, batch_size, visualize_interval, train_per_eps, learning_rate, initial_epsilon, final_epsilon, epsilon_decay_interval, update_interval, eval_interval, eval_eps, agent_id, train_eps, initial_save_threshold, game_id)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mstart_buffer_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         train_reward, epsilon = play_episode(\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mtrain_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mcollect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mepsilon_decay_per_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon_decay_per_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_steps' is not defined"
     ]
    }
   ],
   "source": [
    "train_agent(\n",
    "    game_id='Breakout-v0',\n",
    "    max_eps=2000,\n",
    "    render=False,\n",
    "    batch_size=32,\n",
    "    train_eps=20,\n",
    "    train_per_eps=50000,\n",
    "    max_buffer_len=200000,\n",
    "    start_buffer_len=50000,\n",
    "    eval_eps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
