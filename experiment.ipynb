{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ultronify/atari-dqn/blob/master/experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w58OjrKzOIO9"
   },
   "source": [
    "# Play Atari games with DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RyPM8KWNOIO-"
   },
   "source": [
    "This notebook shows how to play image based Atari games (will leave out the RAM games since the pipeline can be very different) with DQN agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUNZPpFrOVfa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "colab = False\n",
    "\n",
    "base_dir = './'\n",
    "\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    gdrive_dir = '/content/gdrive';\n",
    "    drive.mount(gdrive_dir)\n",
    "    base_dir = os.path.join(gdrive_dir, 'My\\ Drive', 'github_colab_projects', 'dqn-atari')\n",
    "    !pip install tf_agents gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dLDSAorpOIO-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BM7VyonlOIPA"
   },
   "outputs": [],
   "source": [
    "class Qnet(Model):\n",
    "    def __init__(self, action_space_size, state_shape):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.action_space_size = action_space_size\n",
    "        self.state_shape = state_shape\n",
    "        # Define conv layers\n",
    "        self.conv_in = Conv2D(32, 8, strides=4, activation='relu', input_shape=self.state_shape)\n",
    "        self.conv_0 = Conv2D(64, 4, strides=2, activation='relu')\n",
    "        self.conv_1 = Conv2D(64, 3, strides=1, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        # Dense layers\n",
    "        self.dense_0 = Dense(512, activation='relu')\n",
    "        self.dense_out = Dense(self.action_space_size, activation='linear')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.conv_in(x)\n",
    "        x = self.conv_0(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_0(x)\n",
    "        x = self.dense_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YuYRdiKYOIPC"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_length, state_shape, action_space_size):\n",
    "        self.max_length = max_length\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space_size = action_space_size\n",
    "        self.replay_buffer = deque(maxlen=max_length)\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self.replay_buffer)\n",
    "    \n",
    "    def add_batch(self, batch):\n",
    "        self.replay_buffer.append(batch)\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "        states_batch = []\n",
    "        q_vals_batch = []\n",
    "        for s in batch:\n",
    "            state, q_vals = s\n",
    "            states_batch.append(state)\n",
    "            q_vals_batch.append(q_vals)\n",
    "        return tf.convert_to_tensor(states_batch, dtype=tf.float32), tf.convert_to_tensor(q_vals_batch, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmsASrjxOIPE"
   },
   "outputs": [],
   "source": [
    "class DqnAgent:    \n",
    "    def __init__(self, action_space_size, state_shape, train_per_eps,\n",
    "                 learning_rate, checkpoint_location, update_interval):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.state_shape = state_shape\n",
    "        self.update_interval = update_interval\n",
    "        self.batch_cnt = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_per_eps = train_per_eps\n",
    "        self.checkpoint_location = checkpoint_location\n",
    "        self.q_net = self.create_q_net()\n",
    "        self.target_q_net = self.create_q_net()\n",
    "        self.checkpoint = tf.train.Checkpoint(step=tf.Variable(1), net=self.q_net)\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(\n",
    "            self.checkpoint, self.checkpoint_location, max_to_keep=1)\n",
    "        try:\n",
    "            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
    "            self.update()\n",
    "        except:\n",
    "            self.q_net = self.create_q_net()\n",
    "            print('Checkpoint not found, initialize new model')\n",
    "        \n",
    "    def update(self):\n",
    "        self.target_q_net.set_weights(self.q_net.get_weights())\n",
    "        \n",
    "    def create_q_net(self):\n",
    "        q_net = Qnet(self.action_space_size, self.state_shape)\n",
    "        # q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss=tf.keras.losses.Huber())\n",
    "        # q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        q_net.compile(optimizer=tf.optimizers.RMSprop(learning_rate=self.learning_rate), loss='mse')\n",
    "        return q_net\n",
    "    \n",
    "    def random_policy(self, state):\n",
    "        return np.random.randint(self.action_space_size)\n",
    "    \n",
    "    def net_policy(self, state):\n",
    "        q_vals = self.get_q_vals(state).numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        return action\n",
    "    \n",
    "    def collect_policy(self, state, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return self.random_policy(state)\n",
    "        return self.net_policy(state)\n",
    "    \n",
    "    @tf.function\n",
    "    def get_next_q_vals(self, state):\n",
    "        next_q_vals = self.target_q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        return next_q_vals\n",
    "    \n",
    "    @tf.function\n",
    "    def get_q_vals(self, state):\n",
    "        q_vals = self.q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        return q_vals\n",
    "    \n",
    "    def train(self, replay_buffer, batch_size):\n",
    "        loss = []\n",
    "        for i in range(self.train_per_eps):\n",
    "            states, target_q_vals = replay_buffer.get_batch(batch_size)\n",
    "            result = self.q_net.fit(x=states, y=target_q_vals, verbose=1)\n",
    "            loss.append(result.history['loss'])\n",
    "            self.batch_cnt += 1\n",
    "            if self.batch_cnt % self.update_interval == 0:\n",
    "                print('Finished {0} ({1}%). Update target net.'.format(i, i / self.train_per_eps * 100.0))\n",
    "                self.update()\n",
    "            self.checkpoint_manager.save()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7KZ1OSfxOIPG"
   },
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    def __init__(self, game_id, n_frames, max_noop_steps):\n",
    "        self.game_id = game_id\n",
    "        self.n_frames = n_frames\n",
    "        self.max_noop_steps = max_noop_steps\n",
    "        self.env = gym.make(game_id)\n",
    "        self.frames = deque(maxlen=self.n_frames)\n",
    "        self.init_state = None\n",
    "        self.still_doing_nothing = True\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def sanitize_state(self, state, downsample=2):\n",
    "        img = Image.fromarray(state).convert('L')\n",
    "        w, h = img.size\n",
    "        w, h = w // 2, h // 2\n",
    "        img = np.array(img.resize((w, h)))\n",
    "        normalized_img = img / 255.0\n",
    "        return normalized_img\n",
    "    \n",
    "    def normalize_reward(self, reward):\n",
    "        if reward > 0:\n",
    "            return 1.0\n",
    "        elif reward < 0:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "        \n",
    "    def get_action_space_size(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def get_state_shape(self):\n",
    "        initial_state = self.reset()\n",
    "        return initial_state.shape\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "        \n",
    "    def get_frames(self):\n",
    "        frames = np.array(self.frames)\n",
    "        frames = np.swapaxes(frames, 0, 2)\n",
    "        return frames\n",
    "    \n",
    "    def doing_noting(self, state):\n",
    "        if self.current_step == self.max_noop_steps:\n",
    "            return self.still_doing_nothing\n",
    "        elif self.current_step < self.max_noop_steps:\n",
    "            if self.still_doing_nothing and not np.array_equal(np.array(state), self.init_state):\n",
    "                self.still_doing_nothing = False\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def reset(self):\n",
    "        unsanitized_initial_state = self.env.reset()\n",
    "        self.init_state = np.array(unsanitized_initial_state)\n",
    "        self.current_step = 0\n",
    "        self.still_doing_nothing = True\n",
    "        initial_state = self.sanitize_state(unsanitized_initial_state)\n",
    "        for _ in range(self.n_frames):\n",
    "            self.frames.append(initial_state)\n",
    "        return self.get_frames()\n",
    "    \n",
    "    def step(self, action):\n",
    "        unsanitized_state, unnormalized_reward, done, info = self.env.step(action)\n",
    "        if self.doing_noting(unsanitized_state):\n",
    "            done = True\n",
    "        state = self.sanitize_state(unsanitized_state)\n",
    "        self.frames.append(state)\n",
    "        reward = self.normalize_reward(unnormalized_reward)\n",
    "        self.current_step += 1\n",
    "        return self.get_frames(), reward, done, info\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxtMZ411OIPH"
   },
   "outputs": [],
   "source": [
    "def play_episode(env, agent, epsilon, epsilon_decay_per_step, final_epsilon, gamma, collect, buffer, render):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0.0\n",
    "    current_epsilon = epsilon\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.collect_policy(state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if collect:\n",
    "            q_vals = agent.get_q_vals(state).numpy()[0]\n",
    "            next_q_vals = agent.get_next_q_vals(next_state).numpy()[0]\n",
    "            q_vals[action] = reward\n",
    "            if not done:\n",
    "                q_vals[action] += (next_q_vals.max() * gamma)\n",
    "            buffer.add_batch((state, q_vals))\n",
    "            if current_epsilon >= final_epsilon:\n",
    "                current_epsilon -= epsilon_decay_per_step\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    return total_reward, current_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKVdgTmzOIPJ"
   },
   "outputs": [],
   "source": [
    "def init_tensorflow_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    print('GPU initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-X5p992OIPL"
   },
   "outputs": [],
   "source": [
    "def visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps):\n",
    "    fig, (loss_plt, train_plt, eval_plt) = plt.subplots(3, figsize=(20, 8))\n",
    "    fig.suptitle('Eps {0} with epsilon {1}'.format(eps, epsilon))\n",
    "    loss_plt.plot(loss)\n",
    "    loss_plt.set_title('Loss history')\n",
    "    train_plt.plot(train_rewards)\n",
    "    train_plt.set_title('Training reward history')\n",
    "    eval_plt.plot(eval_rewards)\n",
    "    eval_plt.set_title('Eval reward history')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gqxY5bN8OIPN"
   },
   "outputs": [],
   "source": [
    "def train_agent(max_eps=200,\n",
    "                max_steps_increase_amount=10,\n",
    "                max_steps_increase_interval=5,\n",
    "                render=False,\n",
    "                learning_rate=0.00025,\n",
    "                max_buffer_len=10000000,\n",
    "                start_buffer_len=50000,\n",
    "                max_noop_steps=30,\n",
    "                batch_size=128,\n",
    "                visualize_interval=1,\n",
    "                train_per_eps=50000,\n",
    "                initial_epsilon=1.0,\n",
    "                final_epsilon=0.1,\n",
    "                epsilon_decay_interval=1000000,\n",
    "                update_interval=10000,\n",
    "                eval_interval=1,\n",
    "                eval_eps=5,\n",
    "                gamma=0.99,\n",
    "                base_dir='./',\n",
    "                agent_id='default',\n",
    "                train_eps=5,\n",
    "                initial_save_threshold=0.0,\n",
    "                game_id='Breakout-v0'):\n",
    "    init_tensorflow_gpu()\n",
    "    train_env = GameEnv(game_id, 4, max_noop_steps)\n",
    "    eval_env = GameEnv(game_id, 4, max_noop_steps)\n",
    "    state_shape = train_env.get_state_shape()\n",
    "    action_space_size = train_env.get_action_space_size()\n",
    "    epsilon = initial_epsilon\n",
    "    buf = ReplayBuffer(max_buffer_len, state_shape, action_space_size)\n",
    "    print('Initialize agent with state shape {0} and action space size {1}'.format(state_shape, action_space_size))\n",
    "    agent = DqnAgent(action_space_size, state_shape,\n",
    "                     train_per_eps, learning_rate,\n",
    "                     os.path.join(base_dir, 'checkpoints', agent_id),\n",
    "                     update_interval)\n",
    "    loss = []\n",
    "    train_rewards = [0.0]\n",
    "    eval_rewards = [0.0]\n",
    "    best_eval_reward = initial_save_threshold\n",
    "    init_buf_steps = 0\n",
    "    epsilon_decay_per_step = (initial_epsilon - final_epsilon) / epsilon_decay_interval\n",
    "    while buf.get_size() < start_buffer_len:\n",
    "        train_reward, epsilon = play_episode(\n",
    "            train_env,\n",
    "            agent,\n",
    "            render=render,\n",
    "            collect=True,\n",
    "            buffer=buf,\n",
    "            epsilon=epsilon,\n",
    "            gamma=gamma,\n",
    "            epsilon_decay_per_step=epsilon_decay_per_step,\n",
    "            final_epsilon=final_epsilon\n",
    "        )\n",
    "        train_rewards.append(train_reward)\n",
    "        print('Currently filled {0} ({1}%) buffer'.format(buf.get_size(), buf.get_size() / start_buffer_len * 100.0))\n",
    "        init_buf_steps += 1\n",
    "    print('Buffer has been filled with {0} episodes.'.format(init_buf_steps))\n",
    "    for eps in range(1, max_eps + 1):\n",
    "        for i in range(train_eps):\n",
    "            train_reward, epsilon = play_episode(\n",
    "                train_env,\n",
    "                agent,\n",
    "                render=render,\n",
    "                collect=True,\n",
    "                buffer=buf,\n",
    "                epsilon=epsilon,\n",
    "                gamma=gamma,\n",
    "                epsilon_decay_per_step=epsilon_decay_per_step,\n",
    "                final_epsilon=final_epsilon\n",
    "            )\n",
    "            train_rewards.append(train_reward)\n",
    "        print('Finished {0}/{1}'.format(i + 1, train_eps))\n",
    "        loss += agent.train(buf, batch_size)\n",
    "        if eps % eval_interval == 0:\n",
    "            total_reward = 0.0\n",
    "            for _ in range(eval_eps):\n",
    "                eval_reward, _ = play_episode(\n",
    "                    eval_env,\n",
    "                    agent,\n",
    "                    render=render,\n",
    "                    collect=False,\n",
    "                    epsilon=0,\n",
    "                    epsilon_decay_per_step=0,\n",
    "                    final_epsilon=0,\n",
    "                    buffer=None,\n",
    "                    gamma=0\n",
    "                )\n",
    "                total_reward += eval_reward\n",
    "            avg_reward = total_reward / eval_eps\n",
    "            eval_rewards.append(avg_reward)\n",
    "            if avg_reward >= best_eval_reward:\n",
    "                best_eval_reward = avg_reward\n",
    "                tf.saved_model.save(agent.q_net, os.path.join(base_dir, 'models', agent_id))\n",
    "        if eps % visualize_interval == 0:\n",
    "            visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps)\n",
    "    print('Last saved model has eval result {0}'.format(best_eval_reward))\n",
    "    train_env.close()\n",
    "    eval_env.close()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZodMpWS_OIPP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "GPU initialized\n",
      "Initialize agent with state shape (80, 105, 4) and action space size 4\n",
      "Currently filled 168 (0.336%) buffer\n",
      "Currently filled 475 (0.95%) buffer\n",
      "Currently filled 756 (1.512%) buffer\n",
      "Currently filled 976 (1.952%) buffer\n",
      "Currently filled 1159 (2.318%) buffer\n",
      "Currently filled 1434 (2.868%) buffer\n",
      "Currently filled 1685 (3.37%) buffer\n",
      "Currently filled 1860 (3.7199999999999998%) buffer\n",
      "Currently filled 2208 (4.4159999999999995%) buffer\n",
      "Currently filled 2518 (5.0360000000000005%) buffer\n",
      "Currently filled 2866 (5.732%) buffer\n",
      "Currently filled 3156 (6.311999999999999%) buffer\n",
      "Currently filled 3392 (6.784%) buffer\n",
      "Currently filled 3574 (7.148000000000001%) buffer\n",
      "Currently filled 3784 (7.568%) buffer\n",
      "Currently filled 3951 (7.902000000000001%) buffer\n",
      "Currently filled 4219 (8.437999999999999%) buffer\n",
      "Currently filled 4582 (9.164%) buffer\n",
      "Currently filled 4889 (9.778%) buffer\n",
      "Currently filled 5168 (10.335999999999999%) buffer\n",
      "Currently filled 5448 (10.896%) buffer\n",
      "Currently filled 5706 (11.411999999999999%) buffer\n",
      "Currently filled 6052 (12.104%) buffer\n",
      "Currently filled 6225 (12.45%) buffer\n",
      "Currently filled 6538 (13.075999999999999%) buffer\n",
      "Currently filled 6795 (13.59%) buffer\n",
      "Currently filled 7063 (14.126%) buffer\n",
      "Currently filled 7336 (14.671999999999999%) buffer\n",
      "Currently filled 7549 (15.098%) buffer\n",
      "Currently filled 7719 (15.437999999999999%) buffer\n",
      "Currently filled 7982 (15.964%) buffer\n",
      "Currently filled 8150 (16.3%) buffer\n",
      "Currently filled 8343 (16.686%) buffer\n",
      "Currently filled 8548 (17.096%) buffer\n",
      "Currently filled 8825 (17.65%) buffer\n",
      "Currently filled 9063 (18.126%) buffer\n",
      "Currently filled 9491 (18.982%) buffer\n",
      "Currently filled 9701 (19.402%) buffer\n",
      "Currently filled 9878 (19.756%) buffer\n",
      "Currently filled 10192 (20.384%) buffer\n",
      "Currently filled 10425 (20.849999999999998%) buffer\n",
      "Currently filled 10671 (21.342%) buffer\n",
      "Currently filled 10922 (21.844%) buffer\n",
      "Currently filled 11101 (22.201999999999998%) buffer\n",
      "Currently filled 11377 (22.753999999999998%) buffer\n",
      "Currently filled 11646 (23.291999999999998%) buffer\n",
      "Currently filled 11867 (23.733999999999998%) buffer\n",
      "Currently filled 12078 (24.156%) buffer\n",
      "Currently filled 12354 (24.708%) buffer\n",
      "Currently filled 12528 (25.056%) buffer\n",
      "Currently filled 12973 (25.946%) buffer\n",
      "Currently filled 13276 (26.552%) buffer\n",
      "Currently filled 13459 (26.918%) buffer\n",
      "Currently filled 13799 (27.598%) buffer\n",
      "Currently filled 14075 (28.15%) buffer\n",
      "Currently filled 14252 (28.504%) buffer\n",
      "Currently filled 14587 (29.174%) buffer\n",
      "Currently filled 14828 (29.656%) buffer\n",
      "Currently filled 15065 (30.130000000000003%) buffer\n",
      "Currently filled 15310 (30.620000000000005%) buffer\n",
      "Currently filled 15539 (31.078%) buffer\n",
      "Currently filled 15807 (31.613999999999997%) buffer\n",
      "Currently filled 16235 (32.47%) buffer\n",
      "Currently filled 16475 (32.95%) buffer\n",
      "Currently filled 16853 (33.706%) buffer\n",
      "Currently filled 17154 (34.308%) buffer\n",
      "Currently filled 17537 (35.074%) buffer\n",
      "Currently filled 17799 (35.598%) buffer\n",
      "Currently filled 18009 (36.018%) buffer\n",
      "Currently filled 18340 (36.68%) buffer\n",
      "Currently filled 18573 (37.146%) buffer\n",
      "Currently filled 18843 (37.686%) buffer\n",
      "Currently filled 19011 (38.022%) buffer\n",
      "Currently filled 19297 (38.594%) buffer\n",
      "Currently filled 19557 (39.114%) buffer\n",
      "Currently filled 19748 (39.495999999999995%) buffer\n",
      "Currently filled 19948 (39.896%) buffer\n",
      "Currently filled 20120 (40.239999999999995%) buffer\n",
      "Currently filled 20391 (40.782000000000004%) buffer\n",
      "Currently filled 20633 (41.266000000000005%) buffer\n",
      "Currently filled 20840 (41.68%) buffer\n",
      "Currently filled 21114 (42.228%) buffer\n",
      "Currently filled 21325 (42.65%) buffer\n",
      "Currently filled 21494 (42.988%) buffer\n",
      "Currently filled 21763 (43.525999999999996%) buffer\n",
      "Currently filled 22111 (44.222%) buffer\n",
      "Currently filled 22376 (44.751999999999995%) buffer\n",
      "Currently filled 22788 (45.576%) buffer\n",
      "Currently filled 22967 (45.934000000000005%) buffer\n",
      "Currently filled 23318 (46.636%) buffer\n",
      "Currently filled 23570 (47.14%) buffer\n",
      "Currently filled 23861 (47.721999999999994%) buffer\n",
      "Currently filled 24226 (48.452%) buffer\n",
      "Currently filled 24393 (48.786%) buffer\n",
      "Currently filled 24700 (49.4%) buffer\n",
      "Currently filled 25136 (50.27199999999999%) buffer\n",
      "Currently filled 25537 (51.074%) buffer\n",
      "Currently filled 25732 (51.464%) buffer\n",
      "Currently filled 26144 (52.288000000000004%) buffer\n",
      "Currently filled 26456 (52.912000000000006%) buffer\n",
      "Currently filled 26622 (53.244%) buffer\n",
      "Currently filled 26789 (53.578%) buffer\n",
      "Currently filled 27029 (54.05799999999999%) buffer\n",
      "Currently filled 27204 (54.408%) buffer\n",
      "Currently filled 27460 (54.92%) buffer\n",
      "Currently filled 27727 (55.454%) buffer\n",
      "Currently filled 27910 (55.82%) buffer\n",
      "Currently filled 28226 (56.452000000000005%) buffer\n",
      "Currently filled 28429 (56.858%) buffer\n",
      "Currently filled 28609 (57.218%) buffer\n",
      "Currently filled 28851 (57.702%) buffer\n",
      "Currently filled 29109 (58.218%) buffer\n",
      "Currently filled 29287 (58.574000000000005%) buffer\n",
      "Currently filled 29464 (58.928000000000004%) buffer\n",
      "Currently filled 29772 (59.544%) buffer\n",
      "Currently filled 29940 (59.88%) buffer\n",
      "Currently filled 30209 (60.418000000000006%) buffer\n",
      "Currently filled 30450 (60.9%) buffer\n",
      "Currently filled 30613 (61.226%) buffer\n",
      "Currently filled 30903 (61.806000000000004%) buffer\n",
      "Currently filled 31064 (62.12800000000001%) buffer\n",
      "Currently filled 31239 (62.478%) buffer\n",
      "Currently filled 31496 (62.992000000000004%) buffer\n",
      "Currently filled 31753 (63.50599999999999%) buffer\n",
      "Currently filled 31963 (63.926%) buffer\n",
      "Currently filled 32258 (64.51599999999999%) buffer\n",
      "Currently filled 32498 (64.996%) buffer\n",
      "Currently filled 32682 (65.364%) buffer\n",
      "Currently filled 33024 (66.048%) buffer\n",
      "Currently filled 33193 (66.386%) buffer\n",
      "Currently filled 33440 (66.88%) buffer\n",
      "Currently filled 33718 (67.43599999999999%) buffer\n",
      "Currently filled 33947 (67.894%) buffer\n",
      "Currently filled 34112 (68.22399999999999%) buffer\n",
      "Currently filled 34333 (68.66600000000001%) buffer\n",
      "Currently filled 34505 (69.01%) buffer\n",
      "Currently filled 34848 (69.696%) buffer\n",
      "Currently filled 35114 (70.22800000000001%) buffer\n",
      "Currently filled 35484 (70.968%) buffer\n",
      "Currently filled 35711 (71.422%) buffer\n",
      "Currently filled 35951 (71.902%) buffer\n",
      "Currently filled 36113 (72.226%) buffer\n",
      "Currently filled 36354 (72.708%) buffer\n",
      "Currently filled 36557 (73.114%) buffer\n",
      "Currently filled 36785 (73.57000000000001%) buffer\n",
      "Currently filled 37166 (74.332%) buffer\n",
      "Currently filled 37519 (75.03800000000001%) buffer\n",
      "Currently filled 37750 (75.5%) buffer\n",
      "Currently filled 38078 (76.156%) buffer\n",
      "Currently filled 38371 (76.742%) buffer\n",
      "Currently filled 38598 (77.196%) buffer\n",
      "Currently filled 39042 (78.084%) buffer\n",
      "Currently filled 39280 (78.56%) buffer\n",
      "Currently filled 39608 (79.216%) buffer\n",
      "Currently filled 39773 (79.54599999999999%) buffer\n",
      "Currently filled 39998 (79.996%) buffer\n",
      "Currently filled 40238 (80.476%) buffer\n",
      "Currently filled 40416 (80.83200000000001%) buffer\n",
      "Currently filled 40708 (81.416%) buffer\n",
      "Currently filled 40890 (81.78%) buffer\n",
      "Currently filled 41204 (82.408%) buffer\n",
      "Currently filled 41522 (83.044%) buffer\n",
      "Currently filled 41700 (83.39999999999999%) buffer\n",
      "Currently filled 41927 (83.854%) buffer\n",
      "Currently filled 42196 (84.392%) buffer\n",
      "Currently filled 42368 (84.736%) buffer\n",
      "Currently filled 42610 (85.22%) buffer\n",
      "Currently filled 42876 (85.752%) buffer\n",
      "Currently filled 43049 (86.098%) buffer\n",
      "Currently filled 43283 (86.566%) buffer\n",
      "Currently filled 43621 (87.24199999999999%) buffer\n",
      "Currently filled 43796 (87.592%) buffer\n",
      "Currently filled 44003 (88.006%) buffer\n",
      "Currently filled 44170 (88.34%) buffer\n",
      "Currently filled 44444 (88.888%) buffer\n",
      "Currently filled 44724 (89.44800000000001%) buffer\n",
      "Currently filled 45060 (90.12%) buffer\n",
      "Currently filled 45348 (90.696%) buffer\n",
      "Currently filled 45522 (91.044%) buffer\n",
      "Currently filled 45782 (91.56400000000001%) buffer\n",
      "Currently filled 45944 (91.888%) buffer\n",
      "Currently filled 46228 (92.456%) buffer\n",
      "Currently filled 46458 (92.916%) buffer\n",
      "Currently filled 46627 (93.254%) buffer\n",
      "Currently filled 46998 (93.996%) buffer\n",
      "Currently filled 47168 (94.336%) buffer\n",
      "Currently filled 47484 (94.968%) buffer\n",
      "Currently filled 47722 (95.44399999999999%) buffer\n",
      "Currently filled 47888 (95.776%) buffer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently filled 48064 (96.128%) buffer\n",
      "Currently filled 48231 (96.462%) buffer\n",
      "Currently filled 48404 (96.808%) buffer\n",
      "Currently filled 48687 (97.37400000000001%) buffer\n",
      "Currently filled 48860 (97.72%) buffer\n",
      "Currently filled 49093 (98.18599999999999%) buffer\n",
      "Currently filled 49387 (98.774%) buffer\n",
      "Currently filled 49659 (99.318%) buffer\n",
      "Currently filled 49838 (99.676%) buffer\n",
      "Currently filled 50110 (100.22%) buffer\n",
      "Buffer has been filled with 199 episodes.\n",
      "Finished 10/10\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 0.0029\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 0.0022\n",
      "Finished 1 (2.0%). Update target net.\n"
     ]
    }
   ],
   "source": [
    "train_agent(\n",
    "    game_id='Breakout-v0',\n",
    "    max_eps=2000,\n",
    "    render=False,\n",
    "    batch_size=3200,\n",
    "    train_eps=10,\n",
    "    base_dir=base_dir,\n",
    "    epsilon_decay_interval=1000000,\n",
    "    train_per_eps=50,\n",
    "    update_interval=2,\n",
    "    max_buffer_len=1000000,\n",
    "    start_buffer_len=50000,\n",
    "    eval_eps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAXC86bcOIPR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "include_colab_link": true,
   "name": "experiment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
