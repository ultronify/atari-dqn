{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Atari games with DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to play image based Atari games (will leave out the RAM games since the pipeline can be very different) with DQN agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(Model):\n",
    "    def __init__(self, action_space_size, state_shape):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.action_space_size = action_space_size\n",
    "        self.state_shape = state_shape\n",
    "        # Define conv layers\n",
    "        self.conv_in = Conv2D(32, 8, strides=4, activation='relu', input_shape=self.state_shape)\n",
    "        self.conv_0 = Conv2D(64, 4, strides=2, activation='relu')\n",
    "        self.conv_1 = Conv2D(64, 3, strides=1, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        # Dense layers\n",
    "        self.dense_0 = Dense(512, activation='relu')\n",
    "        self.dense_out = Dense(self.action_space_size, activation='linear')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.conv_in(x)\n",
    "        x = self.conv_0(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_0(x)\n",
    "        x = self.dense_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_length, state_shape, action_space_size):\n",
    "        self.max_length = max_length\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space_size = action_space_size\n",
    "        self.replay_buffer = deque(maxlen=max_length)\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self.replay_buffer)\n",
    "    \n",
    "    def add_batch(self, batch):\n",
    "        self.replay_buffer.append(batch)\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "        states_batch = []\n",
    "        q_vals_batch = []\n",
    "        for s in batch:\n",
    "            state, q_vals = s\n",
    "            states_batch.append(state)\n",
    "            q_vals_batch.append(q_vals)\n",
    "        return tf.convert_to_tensor(states_batch, dtype=tf.float32), tf.convert_to_tensor(q_vals_batch, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent:    \n",
    "    def __init__(self, action_space_size, state_shape, train_per_eps,\n",
    "                 learning_rate, checkpoint_location, update_interval):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.state_shape = state_shape\n",
    "        self.update_interval = update_interval\n",
    "        self.batch_cnt = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_per_eps = train_per_eps\n",
    "        self.checkpoint_location = checkpoint_location\n",
    "        self.q_net = self.create_q_net()\n",
    "        self.target_q_net = self.create_q_net()\n",
    "        self.checkpoint = tf.train.Checkpoint(step=tf.Variable(1), net=self.q_net)\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(\n",
    "            self.checkpoint, self.checkpoint_location, max_to_keep=3)\n",
    "        try:\n",
    "            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
    "            self.update()\n",
    "        except:\n",
    "            self.q_net = self.create_q_net()\n",
    "            print('Checkpoint not found, initialize new model')\n",
    "        \n",
    "    def update(self):\n",
    "        self.target_q_net.set_weights(self.q_net.get_weights())\n",
    "        \n",
    "    def create_q_net(self):\n",
    "        q_net = Qnet(self.action_space_size, self.state_shape)\n",
    "        # q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss=tf.keras.losses.Huber())\n",
    "        # q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        q_net.compile(optimizer=tf.optimizers.RMSprop(learning_rate=self.learning_rate), loss='mse')\n",
    "        return q_net\n",
    "    \n",
    "    def random_policy(self, state):\n",
    "        return np.random.randint(self.action_space_size)\n",
    "    \n",
    "    def net_policy(self, state):\n",
    "        q_vals = self.get_q_vals(state).numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        return action\n",
    "    \n",
    "    def collect_policy(self, state, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return self.random_policy(state)\n",
    "        return self.net_policy(state)\n",
    "    \n",
    "    def get_next_q_vals(self, state):\n",
    "        next_q_vals = self.target_q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        return next_q_vals\n",
    "    \n",
    "    def get_q_vals(self, state):\n",
    "        q_vals = self.q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        return q_vals\n",
    "    \n",
    "    def train(self, replay_buffer, batch_size):\n",
    "        loss = []\n",
    "        for i in range(self.train_per_eps):\n",
    "            states, target_q_vals = replay_buffer.get_batch(batch_size)\n",
    "            result = self.q_net.fit(x=states, y=target_q_vals, verbose=0)\n",
    "            loss.append(result.history['loss'])\n",
    "            self.batch_cnt += 1\n",
    "            if self.batch_cnt % self.update_interval == 0:\n",
    "                print('Finished {0} ({1}%). Update target net.'.format(i, i / self.train_per_eps * 100.0))\n",
    "                self.update()\n",
    "        self.checkpoint_manager.save()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    def __init__(self, game_id, n_frames, max_noop_steps):\n",
    "        self.game_id = game_id\n",
    "        self.n_frames = n_frames\n",
    "        self.max_noop_steps = max_noop_steps\n",
    "        self.env = gym.make(game_id)\n",
    "        self.frames = deque(maxlen=self.n_frames)\n",
    "        self.init_state = None\n",
    "        self.still_doing_nothing = True\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def sanitize_state(self, state, downsample=2):\n",
    "        img = Image.fromarray(state).convert('L')\n",
    "        w, h = img.size\n",
    "        w, h = w // 2, h // 2\n",
    "        img = np.array(img.resize((w, h)))\n",
    "        normalized_img = img / 255.0\n",
    "        return normalized_img\n",
    "    \n",
    "    def normalize_reward(self, reward):\n",
    "        if reward > 0:\n",
    "            return 1.0\n",
    "        elif reward < 0:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "        \n",
    "    def get_action_space_size(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def get_state_shape(self):\n",
    "        initial_state = self.reset()\n",
    "        return initial_state.shape\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "        \n",
    "    def get_frames(self):\n",
    "        frames = np.array(self.frames)\n",
    "        frames = np.swapaxes(frames, 0, 2)\n",
    "        return frames\n",
    "    \n",
    "    def doing_noting(self, state):\n",
    "        if self.current_step == self.max_noop_steps:\n",
    "            return self.still_doing_nothing\n",
    "        elif self.current_step < self.max_noop_steps:\n",
    "            if self.still_doing_nothing and not np.array_equal(np.array(state), self.init_state):\n",
    "                self.still_doing_nothing = False\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def reset(self):\n",
    "        unsanitized_initial_state = self.env.reset()\n",
    "        self.init_state = np.array(unsanitized_initial_state)\n",
    "        self.current_step = 0\n",
    "        self.still_doing_nothing = True\n",
    "        initial_state = self.sanitize_state(unsanitized_initial_state)\n",
    "        for _ in range(self.n_frames):\n",
    "            self.frames.append(initial_state)\n",
    "        return self.get_frames()\n",
    "    \n",
    "    def step(self, action):\n",
    "        unsanitized_state, unnormalized_reward, done, info = self.env.step(action)\n",
    "        if self.doing_noting(unsanitized_state):\n",
    "            done = True\n",
    "        state = self.sanitize_state(unsanitized_state)\n",
    "        self.frames.append(state)\n",
    "        reward = self.normalize_reward(unnormalized_reward)\n",
    "        self.current_step += 1\n",
    "        return self.get_frames(), reward, done, info\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, agent, epsilon, epsilon_decay_per_step, final_epsilon, gamma, collect, buffer, render):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0.0\n",
    "    current_epsilon = epsilon\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.collect_policy(state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if collect:\n",
    "            q_vals = agent.get_q_vals(state).numpy()[0]\n",
    "            next_q_vals = agent.get_next_q_vals(next_state).numpy()[0]\n",
    "            q_vals[action] = reward\n",
    "            if not done:\n",
    "                q_vals[action] += (next_q_vals.max() * gamma)\n",
    "            buffer.add_batch((state, q_vals))\n",
    "            if current_epsilon >= final_epsilon:\n",
    "                current_epsilon -= epsilon_decay_per_step\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    return total_reward, current_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tensorflow_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    print('GPU initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps):\n",
    "    fig, (loss_plt, train_plt, eval_plt) = plt.subplots(3, figsize=(20, 8))\n",
    "    fig.suptitle('Eps {0} with epsilon {1}'.format(eps, epsilon))\n",
    "    loss_plt.plot(loss)\n",
    "    loss_plt.set_title('Loss history')\n",
    "    train_plt.plot(train_rewards)\n",
    "    train_plt.set_title('Training reward history')\n",
    "    eval_plt.plot(eval_rewards)\n",
    "    eval_plt.set_title('Eval reward history')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(max_eps=200,\n",
    "                max_steps_increase_amount=10,\n",
    "                max_steps_increase_interval=5,\n",
    "                render=False,\n",
    "                max_buffer_len=10000000,\n",
    "                start_buffer_len=50000,\n",
    "                max_noop_steps=30,\n",
    "                batch_size=128,\n",
    "                visualize_interval=1,\n",
    "                train_per_eps=50000,\n",
    "                learning_rate=0.00025,\n",
    "                initial_epsilon=1.0,\n",
    "                final_epsilon=0.1,\n",
    "                epsilon_decay_interval=1000000,\n",
    "                update_interval=10000,\n",
    "                eval_interval=1,\n",
    "                eval_eps=5,\n",
    "                base_dir='./',\n",
    "                agent_id='default',\n",
    "                train_eps=5,\n",
    "                initial_save_threshold=0.0,\n",
    "                game_id='Breakout-v0'):\n",
    "    init_tensorflow_gpu()\n",
    "    train_env = GameEnv(game_id, 4, max_noop_steps)\n",
    "    eval_env = GameEnv(game_id, 4, max_noop_steps)\n",
    "    state_shape = train_env.get_state_shape()\n",
    "    action_space_size = train_env.get_action_space_size()\n",
    "    epsilon = initial_epsilon\n",
    "    buf = ReplayBuffer(max_buffer_len, state_shape, action_space_size)\n",
    "    print('Initialize agent with state shape {0} and action space size {1}'.format(state_shape, action_space_size))\n",
    "    agent = DqnAgent(action_space_size, state_shape,\n",
    "                     train_per_eps, learning_rate,\n",
    "                     os.path.join(base_dir, 'checkpoints', agent_id),\n",
    "                     update_interval)\n",
    "    loss = []\n",
    "    train_rewards = [0.0]\n",
    "    eval_rewards = [0.0]\n",
    "    best_eval_reward = initial_save_threshold\n",
    "    init_buf_steps = 0\n",
    "    epsilon_decay_per_step = (initial_epsilon - final_epsilon) / epsilon_decay_interval\n",
    "    while buf.get_size() < start_buffer_len:\n",
    "        train_reward, epsilon = play_episode(\n",
    "            train_env, agent, render=render,\n",
    "            collect=True, buffer=buf, epsilon=epsilon, gamma=0.95,\n",
    "            epsilon_decay_per_step=epsilon_decay_per_step,\n",
    "            final_epsilon=final_epsilon\n",
    "        )\n",
    "        train_rewards.append(train_reward)\n",
    "        print('Currently filled {0} ({1}%) buffer'.format(buf.get_size(), buf.get_size() / start_buffer_len * 100.0))\n",
    "        init_buf_steps += 1\n",
    "    print('Buffer has been filled with {0} episodes.'.format(init_buf_steps))\n",
    "    for eps in range(1, max_eps + 1):\n",
    "        for _ in range(train_eps):\n",
    "            train_reward, epsilon = play_episode(\n",
    "                train_env, agent, render=render,\n",
    "                collect=True, buffer=buf, epsilon=epsilon, gamma=0.95,\n",
    "                epsilon_decay_per_step=epsilon_decay_per_step,\n",
    "                final_epsilon=final_epsilon\n",
    "            )\n",
    "            train_rewards.append(train_reward)\n",
    "        loss += agent.train(buf, batch_size)\n",
    "        if eps % eval_interval == 0:\n",
    "            total_reward = 0.0\n",
    "            for _ in range(eval_eps):\n",
    "                eval_reward, _ = play_episode(\n",
    "                    eval_env, agent, render=render,\n",
    "                    collect=False, epsilon=0, epsilon_decay_per_step=0,\n",
    "                    final_epsilon=0, buffer=None, gamma=0\n",
    "                )\n",
    "                total_reward += eval_reward\n",
    "            avg_reward = total_reward/eval_eps\n",
    "            eval_rewards.append(avg_reward)\n",
    "            if avg_reward >= best_eval_reward:\n",
    "                best_eval_reward = avg_reward\n",
    "                tf.saved_model.save(agent.q_net, os.path.join(base_dir, 'models', agent_id))\n",
    "        if eps % visualize_interval == 0:\n",
    "            visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps)\n",
    "    print('Last saved model has eval result {0}'.format(best_eval_reward))\n",
    "    train_env.close()\n",
    "    eval_env.close()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "GPU initialized\n",
      "Initialize agent with state shape (80, 105, 4) and action space size 4\n",
      "Currently filled 217 (0.434%) buffer\n",
      "Currently filled 463 (0.9259999999999999%) buffer\n",
      "Currently filled 782 (1.564%) buffer\n",
      "Currently filled 1079 (2.158%) buffer\n",
      "Currently filled 1352 (2.704%) buffer\n",
      "Currently filled 1639 (3.2779999999999996%) buffer\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-414c6ff0482d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# In the paper this is 50000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mstart_buffer_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     eval_eps=5)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-489e7fd18b8a>\u001b[0m in \u001b[0;36mtrain_agent\u001b[1;34m(max_eps, max_steps_increase_amount, max_steps_increase_interval, render, max_buffer_len, start_buffer_len, max_noop_steps, batch_size, visualize_interval, train_per_eps, learning_rate, initial_epsilon, final_epsilon, epsilon_decay_interval, update_interval, eval_interval, eval_eps, base_dir, agent_id, train_eps, initial_save_threshold, game_id)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mcollect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mepsilon_decay_per_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon_decay_per_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mfinal_epsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinal_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         )\n\u001b[0;32m     48\u001b[0m         \u001b[0mtrain_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-c323a3b5413b>\u001b[0m in \u001b[0;36mplay_episode\u001b[1;34m(env, agent, epsilon, epsilon_decay_per_step, final_epsilon, gamma, collect, buffer, render)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcollect\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mq_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_q_vals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mnext_q_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next_q_vals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mq_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-188db3e6cc32>\u001b[0m in \u001b[0;36mget_q_vals\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_q_vals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mq_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_vals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-296a5449aa0b>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_in\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    245\u001b[0m       \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1016\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1018\u001b[1;33m       name=name)\n\u001b[0m\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[0;32m   1146\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m   1149\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   2590\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2591\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2592\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m   2593\u001b[0m   return squeeze_batch_dims(\n\u001b[0;32m   2594\u001b[0m       \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m    933\u001b[0m         \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_format\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m         \"dilations\", dilations)\n\u001b[0m\u001b[0;32m    936\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "colab = False\n",
    "\n",
    "base_dir = './'\n",
    "\n",
    "if colab:\n",
    "    !pip install tf_agents gym[atari]\n",
    "    from google.colab import drive\n",
    "    gdrive_dir = '/content/gdrive';\n",
    "    drive.mount(gdrive_dir)\n",
    "    base_dir = os.path.join(gdrive_dir, 'github_colab_projects', 'dqn-atari')\n",
    "\n",
    "train_agent(\n",
    "    game_id='Breakout-v0',\n",
    "    max_eps=2000,\n",
    "    render=False,\n",
    "    batch_size=32,\n",
    "    train_eps=20,\n",
    "    base_dir=base_dir,\n",
    "    # In the paper this is 50000\n",
    "    train_per_eps=50000,\n",
    "    max_buffer_len=500000,\n",
    "    # In the paper this is 50000\n",
    "    start_buffer_len=50000,\n",
    "    eval_eps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
