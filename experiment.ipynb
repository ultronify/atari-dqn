{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "experiment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ultronify/atari-dqn/blob/master/experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w58OjrKzOIO9",
        "colab_type": "text"
      },
      "source": [
        "# Play Atari games with DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyPM8KWNOIO-",
        "colab_type": "text"
      },
      "source": [
        "This notebook shows how to play image based Atari games (will leave out the RAM games since the pipeline can be very different) with DQN agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUNZPpFrOVfa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "6f5be004-9229-4437-ff1f-942877ced094"
      },
      "source": [
        "import os\n",
        "\n",
        "colab = True\n",
        "\n",
        "base_dir = './'\n",
        "\n",
        "if colab:\n",
        "    from google.colab import drive\n",
        "    gdrive_dir = '/content/gdrive';\n",
        "    drive.mount(gdrive_dir)\n",
        "    base_dir = os.path.join(gdrive_dir, 'My\\ Drive', 'github_colab_projects', 'dqn-atari')\n",
        "    !pip install tf_agents gym[atari]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: tf_agents in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: tensorflow-probability>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (0.11.0)\n",
            "Requirement already satisfied: gin-config>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (0.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle==1.3 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (0.9.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.11.0->tf_agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.11.0->tf_agents) (0.3.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.11.0->tf_agents) (0.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.3->tf_agents) (49.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLDSAorpOIO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM7VyonlOIPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Qnet(Model):\n",
        "    def __init__(self, action_space_size, state_shape):\n",
        "        super(Qnet, self).__init__()\n",
        "        self.action_space_size = action_space_size\n",
        "        self.state_shape = state_shape\n",
        "        # Define conv layers\n",
        "        self.conv_in = Conv2D(32, 8, strides=4, activation='relu', input_shape=self.state_shape)\n",
        "        self.conv_0 = Conv2D(64, 4, strides=2, activation='relu')\n",
        "        self.conv_1 = Conv2D(64, 3, strides=1, activation='relu')\n",
        "        self.flatten = Flatten()\n",
        "        # Dense layers\n",
        "        self.dense_0 = Dense(512, activation='relu')\n",
        "        self.dense_out = Dense(self.action_space_size, activation='linear')\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        x = self.conv_in(x)\n",
        "        x = self.conv_0(x)\n",
        "        x = self.conv_1(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense_0(x)\n",
        "        x = self.dense_out(x)\n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuYRdiKYOIPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_length, state_shape, action_space_size):\n",
        "        self.max_length = max_length\n",
        "        self.state_shape = state_shape\n",
        "        self.action_space_size = action_space_size\n",
        "        self.replay_buffer = deque(maxlen=max_length)\n",
        "    \n",
        "    def get_size(self):\n",
        "        return len(self.replay_buffer)\n",
        "    \n",
        "    def add_batch(self, batch):\n",
        "        self.replay_buffer.append(batch)\n",
        "    \n",
        "    def get_batch(self, batch_size):\n",
        "        batch = random.sample(self.replay_buffer, batch_size)\n",
        "        states_batch = []\n",
        "        q_vals_batch = []\n",
        "        for s in batch:\n",
        "            state, q_vals = s\n",
        "            states_batch.append(state)\n",
        "            q_vals_batch.append(q_vals)\n",
        "        return tf.convert_to_tensor(states_batch, dtype=tf.float32), tf.convert_to_tensor(q_vals_batch, dtype=tf.float32)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmsASrjxOIPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DqnAgent:    \n",
        "    def __init__(self, action_space_size, state_shape, train_per_eps,\n",
        "                 learning_rate, checkpoint_location, update_interval):\n",
        "        self.action_space_size = action_space_size\n",
        "        self.state_shape = state_shape\n",
        "        self.update_interval = update_interval\n",
        "        self.batch_cnt = 0\n",
        "        self.learning_rate = learning_rate\n",
        "        self.train_per_eps = train_per_eps\n",
        "        self.checkpoint_location = checkpoint_location\n",
        "        self.q_net = self.create_q_net()\n",
        "        self.target_q_net = self.create_q_net()\n",
        "        self.checkpoint = tf.train.Checkpoint(step=tf.Variable(1), net=self.q_net)\n",
        "        self.checkpoint_manager = tf.train.CheckpointManager(\n",
        "            self.checkpoint, self.checkpoint_location, max_to_keep=3)\n",
        "        try:\n",
        "            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
        "            self.update()\n",
        "        except:\n",
        "            self.q_net = self.create_q_net()\n",
        "            print('Checkpoint not found, initialize new model')\n",
        "        \n",
        "    def update(self):\n",
        "        self.target_q_net.set_weights(self.q_net.get_weights())\n",
        "        \n",
        "    def create_q_net(self):\n",
        "        q_net = Qnet(self.action_space_size, self.state_shape)\n",
        "        # q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss=tf.keras.losses.Huber())\n",
        "        # q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        q_net.compile(optimizer=tf.optimizers.RMSprop(learning_rate=self.learning_rate), loss='mse')\n",
        "        return q_net\n",
        "    \n",
        "    def random_policy(self, state):\n",
        "        return np.random.randint(self.action_space_size)\n",
        "    \n",
        "    def net_policy(self, state):\n",
        "        q_vals = self.get_q_vals(state).numpy()[0]\n",
        "        action = np.argmax(q_vals)\n",
        "        return action\n",
        "    \n",
        "    def collect_policy(self, state, epsilon):\n",
        "        if np.random.random() < epsilon:\n",
        "            return self.random_policy(state)\n",
        "        return self.net_policy(state)\n",
        "    \n",
        "    def get_next_q_vals(self, state):\n",
        "        next_q_vals = self.target_q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return next_q_vals\n",
        "    \n",
        "    def get_q_vals(self, state):\n",
        "        q_vals = self.q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return q_vals\n",
        "    \n",
        "    def train(self, replay_buffer, batch_size):\n",
        "        loss = []\n",
        "        for i in range(self.train_per_eps):\n",
        "            states, target_q_vals = replay_buffer.get_batch(batch_size)\n",
        "            result = self.q_net.fit(x=states, y=target_q_vals, verbose=0)\n",
        "            loss.append(result.history['loss'])\n",
        "            self.batch_cnt += 1\n",
        "            if self.batch_cnt % self.update_interval == 0:\n",
        "                print('Finished {0} ({1}%). Update target net.'.format(i, i / self.train_per_eps * 100.0))\n",
        "                self.update()\n",
        "        self.checkpoint_manager.save()\n",
        "        return loss"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KZ1OSfxOIPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GameEnv:\n",
        "    def __init__(self, game_id, n_frames, max_noop_steps):\n",
        "        self.game_id = game_id\n",
        "        self.n_frames = n_frames\n",
        "        self.max_noop_steps = max_noop_steps\n",
        "        self.env = gym.make(game_id)\n",
        "        self.frames = deque(maxlen=self.n_frames)\n",
        "        self.init_state = None\n",
        "        self.still_doing_nothing = True\n",
        "        self.current_step = 0\n",
        "        \n",
        "    def sanitize_state(self, state, downsample=2):\n",
        "        img = Image.fromarray(state).convert('L')\n",
        "        w, h = img.size\n",
        "        w, h = w // 2, h // 2\n",
        "        img = np.array(img.resize((w, h)))\n",
        "        normalized_img = img / 255.0\n",
        "        return normalized_img\n",
        "    \n",
        "    def normalize_reward(self, reward):\n",
        "        if reward > 0:\n",
        "            return 1.0\n",
        "        elif reward < 0:\n",
        "            return -1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "        \n",
        "    def get_action_space_size(self):\n",
        "        return self.env.action_space.n\n",
        "    \n",
        "    def get_state_shape(self):\n",
        "        initial_state = self.reset()\n",
        "        return initial_state.shape\n",
        "        \n",
        "    def render(self):\n",
        "        self.env.render()\n",
        "        \n",
        "    def get_frames(self):\n",
        "        frames = np.array(self.frames)\n",
        "        frames = np.swapaxes(frames, 0, 2)\n",
        "        return frames\n",
        "    \n",
        "    def doing_noting(self, state):\n",
        "        if self.current_step == self.max_noop_steps:\n",
        "            return self.still_doing_nothing\n",
        "        elif self.current_step < self.max_noop_steps:\n",
        "            if self.still_doing_nothing and not np.array_equal(np.array(state), self.init_state):\n",
        "                self.still_doing_nothing = False\n",
        "        else:\n",
        "            return False\n",
        "        \n",
        "    def reset(self):\n",
        "        unsanitized_initial_state = self.env.reset()\n",
        "        self.init_state = np.array(unsanitized_initial_state)\n",
        "        self.current_step = 0\n",
        "        self.still_doing_nothing = True\n",
        "        initial_state = self.sanitize_state(unsanitized_initial_state)\n",
        "        for _ in range(self.n_frames):\n",
        "            self.frames.append(initial_state)\n",
        "        return self.get_frames()\n",
        "    \n",
        "    def step(self, action):\n",
        "        unsanitized_state, unnormalized_reward, done, info = self.env.step(action)\n",
        "        if self.doing_noting(unsanitized_state):\n",
        "            done = True\n",
        "        state = self.sanitize_state(unsanitized_state)\n",
        "        self.frames.append(state)\n",
        "        reward = self.normalize_reward(unnormalized_reward)\n",
        "        self.current_step += 1\n",
        "        return self.get_frames(), reward, done, info\n",
        "    \n",
        "    def close(self):\n",
        "        self.env.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxtMZ411OIPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_episode(env, agent, epsilon, epsilon_decay_per_step, final_epsilon, gamma, collect, buffer, render):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    total_reward = 0.0\n",
        "    current_epsilon = epsilon\n",
        "    while not done:\n",
        "        if render:\n",
        "            env.render()\n",
        "        action = agent.collect_policy(state, epsilon)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        if collect:\n",
        "            q_vals = agent.get_q_vals(state).numpy()[0]\n",
        "            next_q_vals = agent.get_next_q_vals(next_state).numpy()[0]\n",
        "            q_vals[action] = reward\n",
        "            if not done:\n",
        "                q_vals[action] += (next_q_vals.max() * gamma)\n",
        "            buffer.add_batch((state, q_vals))\n",
        "            if current_epsilon >= final_epsilon:\n",
        "                current_epsilon -= epsilon_decay_per_step\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "    return total_reward, current_epsilon"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKVdgTmzOIPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_tensorflow_gpu():\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "    print('GPU initialized')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-X5p992OIPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps):\n",
        "    fig, (loss_plt, train_plt, eval_plt) = plt.subplots(3, figsize=(20, 8))\n",
        "    fig.suptitle('Eps {0} with epsilon {1}'.format(eps, epsilon))\n",
        "    loss_plt.plot(loss)\n",
        "    loss_plt.set_title('Loss history')\n",
        "    train_plt.plot(train_rewards)\n",
        "    train_plt.set_title('Training reward history')\n",
        "    eval_plt.plot(eval_rewards)\n",
        "    eval_plt.set_title('Eval reward history')\n",
        "    plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqxY5bN8OIPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_agent(max_eps=200,\n",
        "                max_steps_increase_amount=10,\n",
        "                max_steps_increase_interval=5,\n",
        "                render=False,\n",
        "                max_buffer_len=10000000,\n",
        "                start_buffer_len=50000,\n",
        "                max_noop_steps=30,\n",
        "                batch_size=128,\n",
        "                visualize_interval=1,\n",
        "                train_per_eps=50000,\n",
        "                learning_rate=0.00025,\n",
        "                initial_epsilon=1.0,\n",
        "                final_epsilon=0.1,\n",
        "                epsilon_decay_interval=1000000,\n",
        "                update_interval=10000,\n",
        "                eval_interval=1,\n",
        "                eval_eps=5,\n",
        "                base_dir='./',\n",
        "                agent_id='default',\n",
        "                train_eps=5,\n",
        "                initial_save_threshold=0.0,\n",
        "                game_id='Breakout-v0'):\n",
        "    init_tensorflow_gpu()\n",
        "    train_env = GameEnv(game_id, 4, max_noop_steps)\n",
        "    eval_env = GameEnv(game_id, 4, max_noop_steps)\n",
        "    state_shape = train_env.get_state_shape()\n",
        "    action_space_size = train_env.get_action_space_size()\n",
        "    epsilon = initial_epsilon\n",
        "    buf = ReplayBuffer(max_buffer_len, state_shape, action_space_size)\n",
        "    print('Initialize agent with state shape {0} and action space size {1}'.format(state_shape, action_space_size))\n",
        "    agent = DqnAgent(action_space_size, state_shape,\n",
        "                     train_per_eps, learning_rate,\n",
        "                     os.path.join(base_dir, 'checkpoints', agent_id),\n",
        "                     update_interval)\n",
        "    loss = []\n",
        "    train_rewards = [0.0]\n",
        "    eval_rewards = [0.0]\n",
        "    best_eval_reward = initial_save_threshold\n",
        "    init_buf_steps = 0\n",
        "    epsilon_decay_per_step = (initial_epsilon - final_epsilon) / epsilon_decay_interval\n",
        "    while buf.get_size() < start_buffer_len:\n",
        "        train_reward, epsilon = play_episode(\n",
        "            train_env, agent, render=render,\n",
        "            collect=True, buffer=buf, epsilon=epsilon, gamma=0.95,\n",
        "            epsilon_decay_per_step=epsilon_decay_per_step,\n",
        "            final_epsilon=final_epsilon\n",
        "        )\n",
        "        train_rewards.append(train_reward)\n",
        "        print('Currently filled {0} ({1}%) buffer'.format(buf.get_size(), buf.get_size() / start_buffer_len * 100.0))\n",
        "        init_buf_steps += 1\n",
        "    print('Buffer has been filled with {0} episodes.'.format(init_buf_steps))\n",
        "    for eps in range(1, max_eps + 1):\n",
        "        for _ in range(train_eps):\n",
        "            train_reward, epsilon = play_episode(\n",
        "                train_env, agent, render=render,\n",
        "                collect=True, buffer=buf, epsilon=epsilon, gamma=0.95,\n",
        "                epsilon_decay_per_step=epsilon_decay_per_step,\n",
        "                final_epsilon=final_epsilon\n",
        "            )\n",
        "            train_rewards.append(train_reward)\n",
        "        loss += agent.train(buf, batch_size)\n",
        "        if eps % eval_interval == 0:\n",
        "            total_reward = 0.0\n",
        "            for _ in range(eval_eps):\n",
        "                eval_reward, _ = play_episode(\n",
        "                    eval_env, agent, render=render,\n",
        "                    collect=False, epsilon=0, epsilon_decay_per_step=0,\n",
        "                    final_epsilon=0, buffer=None, gamma=0\n",
        "                )\n",
        "                total_reward += eval_reward\n",
        "            avg_reward = total_reward/eval_eps\n",
        "            eval_rewards.append(avg_reward)\n",
        "            if avg_reward >= best_eval_reward:\n",
        "                best_eval_reward = avg_reward\n",
        "                tf.saved_model.save(agent.q_net, os.path.join(base_dir, 'models', agent_id))\n",
        "        if eps % visualize_interval == 0:\n",
        "            visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps)\n",
        "    print('Last saved model has eval result {0}'.format(best_eval_reward))\n",
        "    train_env.close()\n",
        "    eval_env.close()\n",
        "    print('Done')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZodMpWS_OIPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "551eeb18-bb59-4986-cd2b-06a0b4f54fcb"
      },
      "source": [
        "train_agent(\n",
        "    game_id='Breakout-v0',\n",
        "    max_eps=2000,\n",
        "    render=False,\n",
        "    batch_size=32,\n",
        "    train_eps=20,\n",
        "    base_dir=base_dir,\n",
        "    epsilon_decay_interval=1000000,\n",
        "    train_per_eps=50000,\n",
        "    max_buffer_len=400000,\n",
        "    start_buffer_len=30000,\n",
        "    eval_eps=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU initialized\n",
            "Initialize agent with state shape (80, 105, 4) and action space size 4\n",
            "Currently filled 177 (0.59%) buffer\n",
            "Currently filled 348 (1.16%) buffer\n",
            "Currently filled 687 (2.29%) buffer\n",
            "Currently filled 876 (2.92%) buffer\n",
            "Currently filled 1052 (3.506666666666667%) buffer\n",
            "Currently filled 1291 (4.303333333333334%) buffer\n",
            "Currently filled 1599 (5.33%) buffer\n",
            "Currently filled 1787 (5.956666666666666%) buffer\n",
            "Currently filled 1978 (6.593333333333333%) buffer\n",
            "Currently filled 2389 (7.963333333333333%) buffer\n",
            "Currently filled 2661 (8.870000000000001%) buffer\n",
            "Currently filled 2903 (9.676666666666666%) buffer\n",
            "Currently filled 3156 (10.52%) buffer\n",
            "Currently filled 3326 (11.086666666666666%) buffer\n",
            "Currently filled 3760 (12.533333333333333%) buffer\n",
            "Currently filled 4136 (13.786666666666667%) buffer\n",
            "Currently filled 4353 (14.510000000000002%) buffer\n",
            "Currently filled 4529 (15.096666666666666%) buffer\n",
            "Currently filled 4840 (16.133333333333333%) buffer\n",
            "Currently filled 5113 (17.043333333333333%) buffer\n",
            "Currently filled 5423 (18.076666666666664%) buffer\n",
            "Currently filled 5659 (18.863333333333333%) buffer\n",
            "Currently filled 5855 (19.516666666666666%) buffer\n",
            "Currently filled 6122 (20.406666666666666%) buffer\n",
            "Currently filled 6305 (21.016666666666666%) buffer\n",
            "Currently filled 6492 (21.64%) buffer\n",
            "Currently filled 6720 (22.400000000000002%) buffer\n",
            "Currently filled 6956 (23.186666666666667%) buffer\n",
            "Currently filled 7121 (23.736666666666668%) buffer\n",
            "Currently filled 7412 (24.706666666666667%) buffer\n",
            "Currently filled 7593 (25.31%) buffer\n",
            "Currently filled 7918 (26.393333333333334%) buffer\n",
            "Currently filled 8084 (26.94666666666667%) buffer\n",
            "Currently filled 8313 (27.71%) buffer\n",
            "Currently filled 8499 (28.33%) buffer\n",
            "Currently filled 8666 (28.886666666666667%) buffer\n",
            "Currently filled 8842 (29.473333333333336%) buffer\n",
            "Currently filled 9139 (30.46333333333333%) buffer\n",
            "Currently filled 9318 (31.06%) buffer\n",
            "Currently filled 9759 (32.53%) buffer\n",
            "Currently filled 10162 (33.873333333333335%) buffer\n",
            "Currently filled 10346 (34.486666666666665%) buffer\n",
            "Currently filled 10518 (35.06%) buffer\n",
            "Currently filled 10693 (35.64333333333333%) buffer\n",
            "Currently filled 10937 (36.45666666666666%) buffer\n",
            "Currently filled 11113 (37.04333333333334%) buffer\n",
            "Currently filled 11294 (37.64666666666667%) buffer\n",
            "Currently filled 11511 (38.37%) buffer\n",
            "Currently filled 11795 (39.31666666666666%) buffer\n",
            "Currently filled 12084 (40.28%) buffer\n",
            "Currently filled 12334 (41.11333333333334%) buffer\n",
            "Currently filled 12548 (41.82666666666667%) buffer\n",
            "Currently filled 12719 (42.39666666666667%) buffer\n",
            "Currently filled 12880 (42.93333333333334%) buffer\n",
            "Currently filled 13104 (43.68%) buffer\n",
            "Currently filled 13358 (44.526666666666664%) buffer\n",
            "Currently filled 13695 (45.65%) buffer\n",
            "Currently filled 13935 (46.45%) buffer\n",
            "Currently filled 14129 (47.096666666666664%) buffer\n",
            "Currently filled 14532 (48.44%) buffer\n",
            "Currently filled 14802 (49.34%) buffer\n",
            "Currently filled 14976 (49.919999999999995%) buffer\n",
            "Currently filled 15209 (50.696666666666665%) buffer\n",
            "Currently filled 15476 (51.58666666666667%) buffer\n",
            "Currently filled 15756 (52.52%) buffer\n",
            "Currently filled 15967 (53.223333333333336%) buffer\n",
            "Currently filled 16327 (54.42333333333333%) buffer\n",
            "Currently filled 16512 (55.04%) buffer\n",
            "Currently filled 16702 (55.67333333333333%) buffer\n",
            "Currently filled 16967 (56.556666666666665%) buffer\n",
            "Currently filled 17234 (57.446666666666665%) buffer\n",
            "Currently filled 17470 (58.233333333333334%) buffer\n",
            "Currently filled 17637 (58.79%) buffer\n",
            "Currently filled 17818 (59.39333333333333%) buffer\n",
            "Currently filled 17980 (59.93333333333334%) buffer\n",
            "Currently filled 18223 (60.74333333333334%) buffer\n",
            "Currently filled 18429 (61.42999999999999%) buffer\n",
            "Currently filled 18612 (62.03999999999999%) buffer\n",
            "Currently filled 18859 (62.86333333333334%) buffer\n",
            "Currently filled 19174 (63.913333333333334%) buffer\n",
            "Currently filled 19353 (64.51%) buffer\n",
            "Currently filled 19622 (65.40666666666667%) buffer\n",
            "Currently filled 19911 (66.36999999999999%) buffer\n",
            "Currently filled 20212 (67.37333333333333%) buffer\n",
            "Currently filled 20550 (68.5%) buffer\n",
            "Currently filled 20718 (69.06%) buffer\n",
            "Currently filled 20931 (69.77%) buffer\n",
            "Currently filled 21107 (70.35666666666667%) buffer\n",
            "Currently filled 21348 (71.16%) buffer\n",
            "Currently filled 21608 (72.02666666666667%) buffer\n",
            "Currently filled 21938 (73.12666666666667%) buffer\n",
            "Currently filled 22205 (74.01666666666667%) buffer\n",
            "Currently filled 22563 (75.21%) buffer\n",
            "Currently filled 22856 (76.18666666666667%) buffer\n",
            "Currently filled 23081 (76.93666666666667%) buffer\n",
            "Currently filled 23253 (77.51%) buffer\n",
            "Currently filled 23618 (78.72666666666667%) buffer\n",
            "Currently filled 23996 (79.98666666666666%) buffer\n",
            "Currently filled 24280 (80.93333333333334%) buffer\n",
            "Currently filled 24499 (81.66333333333333%) buffer\n",
            "Currently filled 24870 (82.89999999999999%) buffer\n",
            "Currently filled 25063 (83.54333333333334%) buffer\n",
            "Currently filled 25411 (84.70333333333333%) buffer\n",
            "Currently filled 25596 (85.32%) buffer\n",
            "Currently filled 25954 (86.51333333333334%) buffer\n",
            "Currently filled 26231 (87.43666666666667%) buffer\n",
            "Currently filled 26636 (88.78666666666668%) buffer\n",
            "Currently filled 26803 (89.34333333333333%) buffer\n",
            "Currently filled 26972 (89.90666666666667%) buffer\n",
            "Currently filled 27136 (90.45333333333333%) buffer\n",
            "Currently filled 27314 (91.04666666666667%) buffer\n",
            "Currently filled 27542 (91.80666666666667%) buffer\n",
            "Currently filled 27714 (92.38%) buffer\n",
            "Currently filled 27990 (93.30000000000001%) buffer\n",
            "Currently filled 28351 (94.50333333333333%) buffer\n",
            "Currently filled 28636 (95.45333333333333%) buffer\n",
            "Currently filled 28812 (96.04%) buffer\n",
            "Currently filled 29104 (97.01333333333334%) buffer\n",
            "Currently filled 29267 (97.55666666666667%) buffer\n",
            "Currently filled 29543 (98.47666666666667%) buffer\n",
            "Currently filled 29729 (99.09666666666666%) buffer\n",
            "Currently filled 30164 (100.54666666666667%) buffer\n",
            "Buffer has been filled with 122 episodes.\n",
            "Finished 9999 (19.997999999999998%). Update target net.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAXC86bcOIPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}