{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "experiment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ultronify/atari-dqn/blob/master/experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w58OjrKzOIO9",
        "colab_type": "text"
      },
      "source": [
        "# Play Atari games with DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyPM8KWNOIO-",
        "colab_type": "text"
      },
      "source": [
        "This notebook shows how to play image based Atari games (will leave out the RAM games since the pipeline can be very different) with DQN agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUNZPpFrOVfa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "222f4439-0bdd-461a-e67b-0636997a0afe"
      },
      "source": [
        "import os\n",
        "\n",
        "colab = True\n",
        "\n",
        "base_dir = './'\n",
        "\n",
        "if colab:\n",
        "    from google.colab import drive\n",
        "    gdrive_dir = '/content/gdrive';\n",
        "    drive.mount(gdrive_dir)\n",
        "    base_dir = os.path.join(gdrive_dir, 'My\\ Drive', 'github_colab_projects', 'dqn-atari')\n",
        "    !pip install tf_agents gym[atari]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: tf_agents in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: cloudpickle==1.3 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (1.3.0)\n",
            "Requirement already satisfied: gin-config>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (0.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf_agents) (1.18.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.3->tf_agents) (49.2.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.11.0->tf_agents) (0.1.5)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.11.0->tf_agents) (0.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.11.0->tf_agents) (4.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLDSAorpOIO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM7VyonlOIPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Qnet(Model):\n",
        "    def __init__(self, action_space_size, state_shape):\n",
        "        super(Qnet, self).__init__()\n",
        "        self.action_space_size = action_space_size\n",
        "        self.state_shape = state_shape\n",
        "        # Define conv layers\n",
        "        self.conv_in = Conv2D(32, 8, strides=4, activation='relu', input_shape=self.state_shape)\n",
        "        self.conv_0 = Conv2D(64, 4, strides=2, activation='relu')\n",
        "        self.conv_1 = Conv2D(64, 3, strides=1, activation='relu')\n",
        "        self.flatten = Flatten()\n",
        "        # Dense layers\n",
        "        self.dense_0 = Dense(512, activation='relu')\n",
        "        self.dense_out = Dense(self.action_space_size, activation='linear')\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        x = self.conv_in(x)\n",
        "        x = self.conv_0(x)\n",
        "        x = self.conv_1(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense_0(x)\n",
        "        x = self.dense_out(x)\n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuYRdiKYOIPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_length, state_shape, action_space_size):\n",
        "        self.max_length = max_length\n",
        "        self.state_shape = state_shape\n",
        "        self.action_space_size = action_space_size\n",
        "        self.replay_buffer = deque(maxlen=max_length)\n",
        "    \n",
        "    def get_size(self):\n",
        "        return len(self.replay_buffer)\n",
        "    \n",
        "    def add_batch(self, batch):\n",
        "        self.replay_buffer.append(batch)\n",
        "    \n",
        "    def get_batch(self, batch_size):\n",
        "        batch = random.sample(self.replay_buffer, batch_size)\n",
        "        states_batch = []\n",
        "        q_vals_batch = []\n",
        "        for s in batch:\n",
        "            state, q_vals = s\n",
        "            states_batch.append(state)\n",
        "            q_vals_batch.append(q_vals)\n",
        "        return tf.convert_to_tensor(states_batch, dtype=tf.float32), tf.convert_to_tensor(q_vals_batch, dtype=tf.float32)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmsASrjxOIPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DqnAgent:    \n",
        "    def __init__(self, action_space_size, state_shape, train_per_eps,\n",
        "                 learning_rate, checkpoint_location, update_interval):\n",
        "        self.action_space_size = action_space_size\n",
        "        self.state_shape = state_shape\n",
        "        self.update_interval = update_interval\n",
        "        self.batch_cnt = 0\n",
        "        self.learning_rate = learning_rate\n",
        "        self.train_per_eps = train_per_eps\n",
        "        self.checkpoint_location = checkpoint_location\n",
        "        self.q_net = self.create_q_net()\n",
        "        self.target_q_net = self.create_q_net()\n",
        "        self.checkpoint = tf.train.Checkpoint(step=tf.Variable(1), net=self.q_net)\n",
        "        self.checkpoint_manager = tf.train.CheckpointManager(\n",
        "            self.checkpoint, self.checkpoint_location, max_to_keep=3)\n",
        "        try:\n",
        "            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
        "            self.update()\n",
        "        except:\n",
        "            self.q_net = self.create_q_net()\n",
        "            print('Checkpoint not found, initialize new model')\n",
        "        \n",
        "    def update(self):\n",
        "        self.target_q_net.set_weights(self.q_net.get_weights())\n",
        "        \n",
        "    def create_q_net(self):\n",
        "        q_net = Qnet(self.action_space_size, self.state_shape)\n",
        "        # q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss=tf.keras.losses.Huber())\n",
        "        # q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        q_net.compile(optimizer=tf.optimizers.RMSprop(learning_rate=self.learning_rate), loss='mse')\n",
        "        return q_net\n",
        "    \n",
        "    def random_policy(self, state):\n",
        "        return np.random.randint(self.action_space_size)\n",
        "    \n",
        "    def net_policy(self, state):\n",
        "        q_vals = self.get_q_vals(state).numpy()[0]\n",
        "        action = np.argmax(q_vals)\n",
        "        return action\n",
        "    \n",
        "    def collect_policy(self, state, epsilon):\n",
        "        if np.random.random() < epsilon:\n",
        "            return self.random_policy(state)\n",
        "        return self.net_policy(state)\n",
        "    \n",
        "    @tf.function\n",
        "    def get_next_q_vals(self, state):\n",
        "        next_q_vals = self.target_q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return next_q_vals\n",
        "    \n",
        "    @tf.function\n",
        "    def get_q_vals(self, state):\n",
        "        q_vals = self.q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return q_vals\n",
        "    \n",
        "    def train(self, replay_buffer, batch_size):\n",
        "        loss = []\n",
        "        for i in range(self.train_per_eps):\n",
        "            states, target_q_vals = replay_buffer.get_batch(batch_size)\n",
        "            result = self.q_net.fit(x=states, y=target_q_vals, verbose=1)\n",
        "            loss.append(result.history['loss'])\n",
        "            self.batch_cnt += 1\n",
        "            if self.batch_cnt % self.update_interval == 0:\n",
        "                print('Finished {0} ({1}%). Update target net.'.format(i, i / self.train_per_eps * 100.0))\n",
        "                self.update()\n",
        "        self.checkpoint_manager.save()\n",
        "        return loss"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KZ1OSfxOIPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GameEnv:\n",
        "    def __init__(self, game_id, n_frames, max_noop_steps):\n",
        "        self.game_id = game_id\n",
        "        self.n_frames = n_frames\n",
        "        self.max_noop_steps = max_noop_steps\n",
        "        self.env = gym.make(game_id)\n",
        "        self.frames = deque(maxlen=self.n_frames)\n",
        "        self.init_state = None\n",
        "        self.still_doing_nothing = True\n",
        "        self.current_step = 0\n",
        "        \n",
        "    def sanitize_state(self, state, downsample=2):\n",
        "        img = Image.fromarray(state).convert('L')\n",
        "        w, h = img.size\n",
        "        w, h = w // 2, h // 2\n",
        "        img = np.array(img.resize((w, h)))\n",
        "        normalized_img = img / 255.0\n",
        "        return normalized_img\n",
        "    \n",
        "    def normalize_reward(self, reward):\n",
        "        if reward > 0:\n",
        "            return 1.0\n",
        "        elif reward < 0:\n",
        "            return -1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "        \n",
        "    def get_action_space_size(self):\n",
        "        return self.env.action_space.n\n",
        "    \n",
        "    def get_state_shape(self):\n",
        "        initial_state = self.reset()\n",
        "        return initial_state.shape\n",
        "        \n",
        "    def render(self):\n",
        "        self.env.render()\n",
        "        \n",
        "    def get_frames(self):\n",
        "        frames = np.array(self.frames)\n",
        "        frames = np.swapaxes(frames, 0, 2)\n",
        "        return frames\n",
        "    \n",
        "    def doing_noting(self, state):\n",
        "        if self.current_step == self.max_noop_steps:\n",
        "            return self.still_doing_nothing\n",
        "        elif self.current_step < self.max_noop_steps:\n",
        "            if self.still_doing_nothing and not np.array_equal(np.array(state), self.init_state):\n",
        "                self.still_doing_nothing = False\n",
        "        else:\n",
        "            return False\n",
        "        \n",
        "    def reset(self):\n",
        "        unsanitized_initial_state = self.env.reset()\n",
        "        self.init_state = np.array(unsanitized_initial_state)\n",
        "        self.current_step = 0\n",
        "        self.still_doing_nothing = True\n",
        "        initial_state = self.sanitize_state(unsanitized_initial_state)\n",
        "        for _ in range(self.n_frames):\n",
        "            self.frames.append(initial_state)\n",
        "        return self.get_frames()\n",
        "    \n",
        "    def step(self, action):\n",
        "        unsanitized_state, unnormalized_reward, done, info = self.env.step(action)\n",
        "        if self.doing_noting(unsanitized_state):\n",
        "            done = True\n",
        "        state = self.sanitize_state(unsanitized_state)\n",
        "        self.frames.append(state)\n",
        "        reward = self.normalize_reward(unnormalized_reward)\n",
        "        self.current_step += 1\n",
        "        return self.get_frames(), reward, done, info\n",
        "    \n",
        "    def close(self):\n",
        "        self.env.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxtMZ411OIPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_episode(env, agent, epsilon, epsilon_decay_per_step, final_epsilon, gamma, collect, buffer, render):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    total_reward = 0.0\n",
        "    current_epsilon = epsilon\n",
        "    while not done:\n",
        "        if render:\n",
        "            env.render()\n",
        "        action = agent.collect_policy(state, epsilon)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        if collect:\n",
        "            q_vals = agent.get_q_vals(state).numpy()[0]\n",
        "            next_q_vals = agent.get_next_q_vals(next_state).numpy()[0]\n",
        "            q_vals[action] = reward\n",
        "            if not done:\n",
        "                q_vals[action] += (next_q_vals.max() * gamma)\n",
        "            buffer.add_batch((state, q_vals))\n",
        "            if current_epsilon >= final_epsilon:\n",
        "                current_epsilon -= epsilon_decay_per_step\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "    return total_reward, current_epsilon"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKVdgTmzOIPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_tensorflow_gpu():\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "    print('GPU initialized')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-X5p992OIPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps):\n",
        "    fig, (loss_plt, train_plt, eval_plt) = plt.subplots(3, figsize=(20, 8))\n",
        "    fig.suptitle('Eps {0} with epsilon {1}'.format(eps, epsilon))\n",
        "    loss_plt.plot(loss)\n",
        "    loss_plt.set_title('Loss history')\n",
        "    train_plt.plot(train_rewards)\n",
        "    train_plt.set_title('Training reward history')\n",
        "    eval_plt.plot(eval_rewards)\n",
        "    eval_plt.set_title('Eval reward history')\n",
        "    plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqxY5bN8OIPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_agent(max_eps=200,\n",
        "                max_steps_increase_amount=10,\n",
        "                max_steps_increase_interval=5,\n",
        "                render=False,\n",
        "                max_buffer_len=10000000,\n",
        "                start_buffer_len=50000,\n",
        "                max_noop_steps=30,\n",
        "                batch_size=128,\n",
        "                visualize_interval=1,\n",
        "                train_per_eps=50000,\n",
        "                learning_rate=0.00025,\n",
        "                initial_epsilon=1.0,\n",
        "                final_epsilon=0.1,\n",
        "                epsilon_decay_interval=1000000,\n",
        "                update_interval=10000,\n",
        "                eval_interval=1,\n",
        "                eval_eps=5,\n",
        "                base_dir='./',\n",
        "                agent_id='default',\n",
        "                train_eps=5,\n",
        "                initial_save_threshold=0.0,\n",
        "                game_id='Breakout-v0'):\n",
        "    init_tensorflow_gpu()\n",
        "    train_env = GameEnv(game_id, 4, max_noop_steps)\n",
        "    eval_env = GameEnv(game_id, 4, max_noop_steps)\n",
        "    state_shape = train_env.get_state_shape()\n",
        "    action_space_size = train_env.get_action_space_size()\n",
        "    epsilon = initial_epsilon\n",
        "    buf = ReplayBuffer(max_buffer_len, state_shape, action_space_size)\n",
        "    print('Initialize agent with state shape {0} and action space size {1}'.format(state_shape, action_space_size))\n",
        "    agent = DqnAgent(action_space_size, state_shape,\n",
        "                     train_per_eps, learning_rate,\n",
        "                     os.path.join(base_dir, 'checkpoints', agent_id),\n",
        "                     update_interval)\n",
        "    loss = []\n",
        "    train_rewards = [0.0]\n",
        "    eval_rewards = [0.0]\n",
        "    best_eval_reward = initial_save_threshold\n",
        "    init_buf_steps = 0\n",
        "    epsilon_decay_per_step = (initial_epsilon - final_epsilon) / epsilon_decay_interval\n",
        "    while buf.get_size() < start_buffer_len:\n",
        "        train_reward, epsilon = play_episode(\n",
        "            train_env, agent, render=render,\n",
        "            collect=True, buffer=buf, epsilon=epsilon, gamma=0.95,\n",
        "            epsilon_decay_per_step=epsilon_decay_per_step,\n",
        "            final_epsilon=final_epsilon\n",
        "        )\n",
        "        train_rewards.append(train_reward)\n",
        "        print('Currently filled {0} ({1}%) buffer'.format(buf.get_size(), buf.get_size() / start_buffer_len * 100.0))\n",
        "        init_buf_steps += 1\n",
        "    print('Buffer has been filled with {0} episodes.'.format(init_buf_steps))\n",
        "    for eps in range(1, max_eps + 1):\n",
        "        for i in range(train_eps):\n",
        "            train_reward, epsilon = play_episode(\n",
        "                train_env, agent, render=render,\n",
        "                collect=True, buffer=buf, epsilon=epsilon, gamma=0.95,\n",
        "                epsilon_decay_per_step=epsilon_decay_per_step,\n",
        "                final_epsilon=final_epsilon\n",
        "            )\n",
        "            train_rewards.append(train_reward)\n",
        "            print('Finished {0}/{1}'.format(i + 1, train_eps))\n",
        "        loss += agent.train(buf, batch_size)\n",
        "        if eps % eval_interval == 0:\n",
        "            total_reward = 0.0\n",
        "            for _ in range(eval_eps):\n",
        "                eval_reward, _ = play_episode(\n",
        "                    eval_env, agent, render=render,\n",
        "                    collect=False, epsilon=0, epsilon_decay_per_step=0,\n",
        "                    final_epsilon=0, buffer=None, gamma=0\n",
        "                )\n",
        "                total_reward += eval_reward\n",
        "            avg_reward = total_reward/eval_eps\n",
        "            eval_rewards.append(avg_reward)\n",
        "            if avg_reward >= best_eval_reward:\n",
        "                best_eval_reward = avg_reward\n",
        "                tf.saved_model.save(agent.q_net, os.path.join(base_dir, 'models', agent_id))\n",
        "        if eps % visualize_interval == 0:\n",
        "            visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps)\n",
        "    print('Last saved model has eval result {0}'.format(best_eval_reward))\n",
        "    train_env.close()\n",
        "    eval_env.close()\n",
        "    print('Done')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZodMpWS_OIPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "780cad32-0989-4689-f34c-a217058c356a"
      },
      "source": [
        "train_agent(\n",
        "    game_id='Breakout-v0',\n",
        "    max_eps=2000,\n",
        "    render=False,\n",
        "    batch_size=3200,\n",
        "    train_eps=10,\n",
        "    base_dir=base_dir,\n",
        "    epsilon_decay_interval=1000000,\n",
        "    train_per_eps=50,\n",
        "    max_buffer_len=30000,\n",
        "    start_buffer_len=15000,\n",
        "    eval_eps=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU initialized\n",
            "Initialize agent with state shape (80, 105, 4) and action space size 4\n",
            "Currently filled 286 (1.9066666666666665%) buffer\n",
            "Currently filled 472 (3.1466666666666665%) buffer\n",
            "Currently filled 847 (5.6466666666666665%) buffer\n",
            "Currently filled 1095 (7.3%) buffer\n",
            "Currently filled 1358 (9.053333333333333%) buffer\n",
            "Currently filled 1555 (10.366666666666667%) buffer\n",
            "Currently filled 1763 (11.753333333333334%) buffer\n",
            "Currently filled 2093 (13.953333333333335%) buffer\n",
            "Currently filled 2403 (16.02%) buffer\n",
            "Currently filled 2584 (17.226666666666667%) buffer\n",
            "Currently filled 2928 (19.52%) buffer\n",
            "Currently filled 3197 (21.313333333333333%) buffer\n",
            "Currently filled 3510 (23.400000000000002%) buffer\n",
            "Currently filled 3714 (24.759999999999998%) buffer\n",
            "Currently filled 3887 (25.913333333333334%) buffer\n",
            "Currently filled 4266 (28.439999999999998%) buffer\n",
            "Currently filled 4440 (29.599999999999998%) buffer\n",
            "Currently filled 4605 (30.7%) buffer\n",
            "Currently filled 4796 (31.973333333333333%) buffer\n",
            "Currently filled 4970 (33.13333333333333%) buffer\n",
            "Currently filled 5387 (35.913333333333334%) buffer\n",
            "Currently filled 5565 (37.1%) buffer\n",
            "Currently filled 5772 (38.48%) buffer\n",
            "Currently filled 5981 (39.873333333333335%) buffer\n",
            "Currently filled 6284 (41.89333333333333%) buffer\n",
            "Currently filled 6498 (43.32%) buffer\n",
            "Currently filled 6808 (45.38666666666666%) buffer\n",
            "Currently filled 7071 (47.14%) buffer\n",
            "Currently filled 7249 (48.32666666666667%) buffer\n",
            "Currently filled 7484 (49.89333333333333%) buffer\n",
            "Currently filled 7904 (52.693333333333335%) buffer\n",
            "Currently filled 8089 (53.92666666666667%) buffer\n",
            "Currently filled 8472 (56.48%) buffer\n",
            "Currently filled 8629 (57.52666666666667%) buffer\n",
            "Currently filled 9075 (60.5%) buffer\n",
            "Currently filled 9417 (62.78%) buffer\n",
            "Currently filled 9653 (64.35333333333332%) buffer\n",
            "Currently filled 10075 (67.16666666666666%) buffer\n",
            "Currently filled 10257 (68.38%) buffer\n",
            "Currently filled 10433 (69.55333333333333%) buffer\n",
            "Currently filled 10669 (71.12666666666667%) buffer\n",
            "Currently filled 10836 (72.24000000000001%) buffer\n",
            "Currently filled 11119 (74.12666666666667%) buffer\n",
            "Currently filled 11349 (75.66000000000001%) buffer\n",
            "Currently filled 11809 (78.72666666666667%) buffer\n",
            "Currently filled 12079 (80.52666666666667%) buffer\n",
            "Currently filled 12382 (82.54666666666667%) buffer\n",
            "Currently filled 12711 (84.74000000000001%) buffer\n",
            "Currently filled 13043 (86.95333333333333%) buffer\n",
            "Currently filled 13380 (89.2%) buffer\n",
            "Currently filled 13643 (90.95333333333333%) buffer\n",
            "Currently filled 13818 (92.12%) buffer\n",
            "Currently filled 14078 (93.85333333333334%) buffer\n",
            "Currently filled 14338 (95.58666666666666%) buffer\n",
            "Currently filled 14538 (96.92%) buffer\n",
            "Currently filled 14719 (98.12666666666667%) buffer\n",
            "Currently filled 14993 (99.95333333333333%) buffer\n",
            "Currently filled 15235 (101.56666666666668%) buffer\n",
            "Buffer has been filled with 58 episodes.\n",
            "Finished 1/10\n",
            "Finished 2/10\n",
            "Finished 3/10\n",
            "Finished 4/10\n",
            "Finished 5/10\n",
            "Finished 6/10\n",
            "Finished 7/10\n",
            "Finished 8/10\n",
            "Finished 9/10\n",
            "Finished 10/10\n",
            "100/100 [==============================] - 8s 76ms/step - loss: 0.0033\n",
            "100/100 [==============================] - 8s 76ms/step - loss: 0.0023\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 0.0021\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 0.0024\n",
            "100/100 [==============================] - 8s 78ms/step - loss: 0.0022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAXC86bcOIPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}