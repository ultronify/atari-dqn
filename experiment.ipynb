{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Atari games with DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to play image based Atari games (will leave out the RAM games since the pipeline can be very different) with DQN agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(Model):\n",
    "    def __init__(self, action_space_size, state_shape):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.action_space_size = action_space_size\n",
    "        self.state_shape = state_shape\n",
    "        # Define conv layers\n",
    "        self.conv_in = Conv2D(16, 8, strides=(4, 4), activation='relu', input_shape=self.state_shape)\n",
    "        self.conv_0 = Conv2D(32, 4, strides=(2, 2), activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        # Dense layers\n",
    "        self.dense_0 = Dense(256, activation='relu')\n",
    "        self.dense_out = Dense(self.action_space_size, activation='linear')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.conv_in(x)\n",
    "        x = self.conv_0(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_0(x)\n",
    "        x = self.dense_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_length, state_shape, action_space_size):\n",
    "        self.max_length = max_length\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space_size = action_space_size\n",
    "        self.state_spec = tf.TensorSpec(state_shape, dtype=tf.float32, name='state')\n",
    "        self.q_vals_spec = tf.TensorSpec([self.action_space_size], dtype=tf.float32, name='q_vals')\n",
    "        self.buffer = TFUniformReplayBuffer(\n",
    "            data_spec=(self.state_spec, self.q_vals_spec),\n",
    "            batch_size=1,\n",
    "            max_length=self.max_length)\n",
    "    \n",
    "    def add_batch(self, batch):\n",
    "        self.buffer.add_batch(batch)\n",
    "    \n",
    "    def get_next(self, sample_batch_size=32):\n",
    "        return self.buffer.get_next(sample_batch_size=sample_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent:    \n",
    "    def __init__(self, action_space_size, state_shape, train_per_eps, learning_rate, checkpoint_location):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.state_shape = state_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_per_eps = train_per_eps\n",
    "        self.checkpoint_location = checkpoint_location\n",
    "        self.q_net = self.create_q_net()\n",
    "        self.target_q_net = self.create_q_net()\n",
    "        self.checkpoint = tf.train.Checkpoint(step=tf.Variable(1), net=self.q_net)\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(\n",
    "            self.checkpoint, self.checkpoint_location, max_to_keep=3)\n",
    "        try:\n",
    "            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
    "            self.update()\n",
    "        except:\n",
    "            self.q_net = self.create_q_net()\n",
    "            print('Checkpoint not found, initialize new model')\n",
    "        \n",
    "    def update(self):\n",
    "        self.target_q_net.set_weights(self.q_net.get_weights())\n",
    "        \n",
    "    def create_q_net(self):\n",
    "        q_net = Qnet(self.action_space_size, self.state_shape)\n",
    "        q_net.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "                      loss=tf.keras.losses.Huber())\n",
    "        return q_net\n",
    "    \n",
    "    def random_policy(self, state):\n",
    "        return np.random.randint(self.action_space_size)\n",
    "    \n",
    "    def net_policy(self, state):\n",
    "        q_vals = self.get_q_vals(state).numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        return action\n",
    "    \n",
    "    def collect_policy(self, state, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return self.random_policy(state)\n",
    "        return self.net_policy(state)\n",
    "    \n",
    "    def get_next_q_vals(self, state):\n",
    "        next_q_vals = self.target_q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        return next_q_vals\n",
    "    \n",
    "    def get_q_vals(self, state):\n",
    "        q_vals = self.q_net(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        return q_vals\n",
    "    \n",
    "    def train(self, replay_buffer, batch_size):\n",
    "        loss = []\n",
    "        for _ in range(self.train_per_eps):\n",
    "            batch, info = replay_buffer.get_next(batch_size)\n",
    "            states, target_q_vals = batch\n",
    "            result = self.q_net.fit(x=states, y=target_q_vals, verbose=0)\n",
    "            loss.append(result.history['loss'])\n",
    "        self.checkpoint_manager.save()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    \"\"\"\n",
    "    Although OpenAI gym library provides most of the\n",
    "    things we need, the state and reward it emits is\n",
    "    too primative to be used. To keep the state and\n",
    "    reward conversion consistent, we wrap the OpenAI\n",
    "    gym environment in a shim class.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def sanitize_state(state, downsample=2):\n",
    "        img = Image.fromarray(state).convert('L')\n",
    "        w, h = img.size\n",
    "        w, h = w // 2, h // 2\n",
    "        img = np.array(img.resize((w, h)))\n",
    "        normalized_img = img / 255.0\n",
    "        return normalized_img\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_reward(reward):\n",
    "        return reward\n",
    "    \n",
    "    def __init__(self, game_id, n_frames):\n",
    "        self.game_id = game_id\n",
    "        self.n_frames = n_frames\n",
    "        self.env = gym.make(game_id)\n",
    "        self.frames = deque(maxlen=self.n_frames)\n",
    "        \n",
    "    def get_action_space_size(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def get_state_shape(self):\n",
    "        initial_state = self.reset()\n",
    "        return initial_state.shape\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "        \n",
    "    def get_frames(self):\n",
    "        frames = np.array(self.frames)\n",
    "        frames = np.swapaxes(frames, 0, 2)\n",
    "        return frames\n",
    "        \n",
    "    def reset(self):\n",
    "        unsanitized_initial_state = self.env.reset()\n",
    "        initial_state = self.sanitize_state(unsanitized_initial_state)\n",
    "        for _ in range(self.n_frames):\n",
    "            self.frames.append(initial_state)\n",
    "        return self.get_frames()\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "    \n",
    "    def step(self, action):\n",
    "        unsanitized_state, unnormalized_reward, done, info = self.env.step(action)\n",
    "        state = self.sanitize_state(unsanitized_state)\n",
    "        self.frames.append(state)\n",
    "        reward = self.normalize_reward(unnormalized_reward)\n",
    "        return self.get_frames(), reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, agent, epsilon=0.0, gamma=0.9, collect=False, buffer=None, render=False, max_steps=100000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0.0\n",
    "    while not done and steps < max_steps:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.collect_policy(state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if collect:\n",
    "            q_vals = agent.get_q_vals(state).numpy()[0]\n",
    "            next_q_vals = agent.get_next_q_vals(next_state).numpy()[0]\n",
    "            q_vals[action] = reward\n",
    "            if not done:\n",
    "                q_vals[action] += (next_q_vals.max() * gamma)\n",
    "            buffer.add_batch((\n",
    "                tf.convert_to_tensor([state], dtype=tf.float32),\n",
    "                tf.convert_to_tensor([q_vals], dtype=tf.float32)\n",
    "            ))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tensorflow_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    print('GPU initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps):\n",
    "    fig, (loss_plt, train_plt, eval_plt) = plt.subplots(3, figsize=(20, 8))\n",
    "    fig.suptitle('Eps {0} with epsilon {1}'.format(eps, epsilon))\n",
    "    loss_plt.plot(loss)\n",
    "    loss_plt.set_title('Loss history')\n",
    "    train_plt.plot(train_rewards)\n",
    "    train_plt.set_title('Training reward history')\n",
    "    eval_plt.plot(eval_rewards)\n",
    "    eval_plt.set_title('Eval reward history')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(max_eps=200, initial_max_steps=512,\n",
    "                max_steps_increase_amount=10,\n",
    "                max_steps_increase_interval=5,\n",
    "                render=False, max_buffer_len=12000,\n",
    "                batch_size=128, visualize_interval=5,\n",
    "                train_per_eps=10, learning_rate=1e-3,\n",
    "                eval_interval=5, initial_epsilon=0.9,\n",
    "                update_interval=5, epsilon_decay=0.99,\n",
    "                epsilon_decay_interval=10, eval_eps=10,\n",
    "                agent_id='default', initial_save_threshold=0.0):\n",
    "    init_tensorflow_gpu()\n",
    "    train_env = GameEnv('Breakout-v0', 4)\n",
    "    eval_env = GameEnv('Breakout-v0', 4)\n",
    "    state_shape = train_env.get_state_shape()\n",
    "    action_space_size = train_env.get_action_space_size()\n",
    "    epsilon = initial_epsilon\n",
    "    max_steps = initial_max_steps\n",
    "    buf = ReplayBuffer(max_buffer_len, state_shape, action_space_size)\n",
    "    print('Initialize agent with state shape {0} and action space size {1}'.format(state_shape, action_space_size))\n",
    "    agent = DqnAgent(action_space_size, state_shape, train_per_eps, learning_rate, os.path.join('./checkpoints', agent_id))\n",
    "    loss = []\n",
    "    train_rewards = []\n",
    "    eval_rewards = []\n",
    "    best_eval_reward = initial_save_threshold\n",
    "    for eps in range(1, max_eps + 1):\n",
    "        train_rewards.append(\n",
    "            play_episode(\n",
    "                train_env, agent, render=render, max_steps=max_steps,\n",
    "                collect=True,buffer=buf, epsilon=epsilon, gamma=0.95\n",
    "            )\n",
    "        )\n",
    "        loss += agent.train(buf, batch_size)\n",
    "        if eps % eval_interval == 0:\n",
    "            total_reward = 0.0\n",
    "            for _ in range(eval_eps):\n",
    "                total_reward += play_episode(\n",
    "                    eval_env, agent, render=render, max_steps=max_steps,\n",
    "                    collect=False)\n",
    "            avg_reward = total_reward/eval_eps\n",
    "            eval_rewards.append(avg_reward)\n",
    "            if avg_reward >= best_eval_reward:\n",
    "                best_eval_reward = avg_reward\n",
    "                tf.saved_model.save(agent.q_net, os.path.join('./models', agent_id))\n",
    "        if eps % update_interval == 0:\n",
    "            agent.update()\n",
    "        if eps % max_steps_increase_interval == 0:\n",
    "            max_steps += max_steps_increase_amount\n",
    "        if eps % epsilon_decay_interval == 0:\n",
    "            epsilon *= epsilon_decay\n",
    "        if eps % visualize_interval == 0:\n",
    "            visualize_training_result(loss, train_rewards, eval_rewards, epsilon, eps)\n",
    "    print('Last saved model has eval result {0}'.format(best_eval_reward))\n",
    "    train_env.close()\n",
    "    eval_env.close()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "GPU initialized\n"
     ]
    }
   ],
   "source": [
    "train_agent(max_eps=600, render=False, initial_epsilon=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
